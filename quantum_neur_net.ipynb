{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A classification of quantum states with neural networks\n",
    "\n",
    "In quantum physics, all physical quantities of a certain system are described by a quantum state. When a system is composed of two or more parties, such as two or more particles (electrons, atoms, etc...), these can be entangled. This situation underpins quantum correlations between these parties such that we could not study them as independent entities. Figuratively speaking, we could say that two or more entangled particles behave like a whole one and we could not know completely one of them independently from all of the others. In case two or more parties are unentangled, we would say they are separable. \n",
    "\n",
    "Quantum entanglement, first discussed in the first decades of the last century, has become increasingly important in some applications such as quantum cryptography and quantum computers. \n",
    "\n",
    "Even today there are still some aspects of this topic to explore. One of this is to determine exactly whether an unknown quantum state is entangled or not. In this project, we apply artificial neural networks to make a classification of entangled and separable quantum states. The feature space is given by the complex components of quantum states. The targets are binary and can be either entangled (1) or separable (0).\n",
    "\n",
    "First, we focus on a special class of quantum states, Werner states. The entangled/separable decision boundary is determined by the real parameter $p_{sym}$, which varies between 0 and 1, and is located at $p_{sym} = 0.5$. Separable states have $p_{sym} \\geq 0.5$ and entangled states have $p_{sym} < 0.5$.\n",
    "\n",
    "Next, we will deal with general quantum states. Unlike the case of Werner states, features of such states are not a linear function of a single parameter like $p_{sym}$. Generally, the decision boundary between entangled and separable states is a highly non-linear function. As a result, while features of Werner states can be casted in a one-dimensional space, the feature space of general states is high-dimensional.\n",
    "\n",
    "Artificial neural networks are used under supervised approach. First, they learn from a training set made of 20000 states. Next, their classification performance is gauged on a test set composed of 5000 states. The training and the test dataset were artificially generated with *Qutip*, a Python library for quantum computations. States were drawn from a random uniform distribution. \n",
    "\n",
    "Since we artificially sampled data, here we do not face any missing-data nor few-data problem unlike other cases. On the other hand, the classification task of quantum states is generally a high-dimensional problem whose exact solution would involve implementing highly non-linear analytical criteria. Among others, this might come with a large computational cost. Neural networks are known to be universal approximators of any possible continuous function (ideally, at arbitrary precision if the network size were infinite). Given this fact, we aim at checking whether neural networks could provide a less expensive and valid alternative to some analytical criteria. In particular, we are interested to investigate whether artificial neural networks with a relatively small size could detect the boundary decision between entangled and separable states at highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Werner states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to sample Werner states \n",
    "\n",
    "# import Python libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "import random \n",
    "\n",
    "dimspace = 2 # dimensional parameter (the single state represents a qubit, i.e. a two-level system)\n",
    "\n",
    "nrstates_train = 20000 # size of the training set\n",
    "nrstates_test = 5000 # size of the test set\n",
    "\n",
    "# Pauli matrices and identity \n",
    "\n",
    "X = qt.sigmax()\n",
    "Y = qt.sigmay()\n",
    "Z = qt.sigmaz()\n",
    "I = qt.qeye(dimspace)\n",
    "\n",
    "# Function to sample Werner states\n",
    "\n",
    "def sampling_Wernerstates(numberOfStates, dimsingle): \n",
    "\n",
    "\n",
    "    totalDim = dimsingle**2\n",
    "\n",
    "    # Creating empty numpy arrays\n",
    "    states = []\n",
    "    labelsWER = []\n",
    "    psym = []\n",
    "\n",
    "    for k in range(numberOfStates): \n",
    "\n",
    "        p_sym = np.random.uniform(0,1) # drawing p_sym from RANDOM UNIFORM DISTRIBUTION into (0,1)\n",
    "\n",
    "        psym.append(p_sym) # append the value of each p_sym to an empty numpy array \n",
    "\n",
    "        # By definition, a Werner state is entangled when p_sym < 0.5 and is separable otherwise.\n",
    "        # Let us give the binary labels 0 (sep) and 1 (ent) starting from this property. \n",
    "\n",
    "        if p_sym >= 0.5:\n",
    "            labelsWER.append(0) \n",
    "        else:\n",
    "            labelsWER.append(1)\n",
    "\n",
    "        # Generate the features of each quantum state (rho) using the function named \"werner_states\" (see below)\n",
    "        \n",
    "        rho = werner_states(p_sym, dimsingle)\n",
    "\n",
    "        rhoFull = rho.full() # convert rho into a numpy array, mantaining its shape\n",
    "\n",
    "        states.append(rhoFull) # append each state to the empty array densityMatrices\n",
    "\n",
    "\n",
    "\n",
    "    dmElementsList = [] # empty array that will contain the components of each quantum state\n",
    "    for m in range(totalDim):\n",
    "        for n in range(totalDim):\n",
    "            dmElementsList.append('rho(' + str(m) + ',' + str(n) + ')') # name of each column \n",
    "\n",
    "    # pandas dataframes \n",
    "\n",
    "    dfDM = pd.DataFrame(np.reshape(states, (numberOfStates, totalDim**2)), columns = dmElementsList) \n",
    "    dfpsyms = pd.DataFrame(np.reshape(psym, (numberOfStates, 1)), columns = [\"p_sym\"])\n",
    "    dfLabelsWER = pd.DataFrame(np.reshape(labelsWER, (numberOfStates, 1)), columns = [\"State type\"]) \n",
    "\n",
    "    return dfDM, dfpsyms, dfLabelsWER\n",
    "\n",
    "\n",
    "\n",
    "# function to compute the Werner states\n",
    "\n",
    "def werner_states(psym, dimsingle):\n",
    "    \n",
    "    alpha = ((1-2*psym)*dimsingle+1)/(1-2*psym+dimsingle) # coefficient \n",
    "    \n",
    "    zero_matrix = np.zeros(shape = (dimsingle,dimsingle))  # d x d matrix entirely made of 0 \n",
    "    \n",
    "    z_m = qt.Qobj(zero_matrix) # convert the zero_matrix into a quantum object \n",
    "    \n",
    "    P = qt.tensor(z_m*z_m.dag(), z_m*z_m.dag())  # d^2 x d^2 matrix entirely made of 0 \n",
    "    \n",
    "    Id = qt.tensor(qt.qeye(dimsingle), qt.qeye(dimsingle))  # identity matrix in a d^2 x d^2 system\n",
    "    \n",
    "    for i in range(dimsingle): \n",
    "        for j in range(dimsingle):\n",
    "            ket = qt.basis(dimsingle, i) \n",
    "            bra = qt.basis(dimsingle, j) \n",
    "            P += qt.tensor(ket*bra.dag(), bra*ket.dag())  # permutation operator\n",
    "\n",
    "    rho = (Id-alpha*P)/(dimsingle**2-dimsingle*alpha)  # most general form of any Werner state\n",
    "    \n",
    "    return rho\n",
    "\n",
    "\n",
    "werner_train, psym_train, labels_train = sampling_Wernerstates(nrstates_train, dimspace) # training set\n",
    "\n",
    "werner_test, psym_test, labels_test = sampling_Wernerstates(nrstates_test, dimspace) # test set\n",
    "\n",
    "\n",
    "# Training data\n",
    "\n",
    "werner_train.to_csv(\"werner_train.csv\") # features\n",
    "psym_train.to_csv(\"psym_train.csv\") # p_sym value\n",
    "labels_train.to_csv(\"labels_werner_train.csv\") # binary targets 0/1\n",
    "\n",
    "# Test data\n",
    "\n",
    "werner_test.to_csv(\"werner_test.csv\") # features\n",
    "psym_test.to_csv(\"psym_test.csv\") # p_sym value \n",
    "labels_test.to_csv(\"labels_werner_test.csv\") # binary targets 0/1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    "\n",
    "Generally, a quantum state is a $4 \\times 4$ complex matrix, so it has 16 complex components. Each of them should be split into real and imaginary part, which results in 32 features. \n",
    "However, Werner states have a simplified structure, i.e. they have few non-zero real components and no imaginary part. As we will see, drawing the boundary between entangled and separable Werner states is a linear problem. As a result, this task could be carried out with maximal accuracy also with a perceptron, i.e. a neural network with no hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "nrstates_train = 20000 # size of the training set\n",
    "nrstates_test = 5000 # size of the test set\n",
    "\n",
    "# Import the datasets\n",
    "\n",
    "werner_train = pd.read_csv(\"werner_train.csv\") # feature space of the training set\n",
    "psym_train = pd.read_csv(\"psym_train.csv\") # p_sym of the states in the training dataset\n",
    "labels_werner_train = pd.read_csv(\"labels_werner_train.csv\") # labels of the training set\n",
    "\n",
    "werner_test = pd.read_csv(\"werner_test.csv\") # features space of the test set\n",
    "psym_test = pd.read_csv(\"psym_test.csv\") # Import p_sym of the states in the test dataset \n",
    "labels_werner_test = pd.read_csv(\"labels_werner_test.csv\") # labels of the test set\n",
    "\n",
    "# Drop additional columns, added while importing the datasets\n",
    "\n",
    "labels_werner_train = labels_werner_train.drop(columns = [\"Unnamed: 0\"])\n",
    "labels_werner_test = labels_werner_test.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "werner_train = werner_train.drop(columns = [\"Unnamed: 0\"])\n",
    "werner_test = werner_test.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "psym_train = psym_train.drop(columns = [\"Unnamed: 0\"])\n",
    "psym_test = psym_test.drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State type\n",
       "0           1\n",
       "1           0\n",
       "2           0\n",
       "3           1\n",
       "4           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the targets of the training set \n",
    "labels_werner_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State type\n",
       "0           1\n",
       "1           0\n",
       "2           1\n",
       "3           1\n",
       "4           1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the targets of the test set \n",
    "labels_werner_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us check whether the training set and the test set are composed of **balanced classes**. Balanced classes are an important condition for the optimal working of neural networks. Since states of both datasets were drawn from random uniform distribution, we envisage that both training and test set should contain 50\\% entangled and 50\\% separable states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of separable states in the training set = 49.99 %\n",
      "Percentage of entangled states in the training set = 50.01 %\n",
      "Percentage of separable states in the test set = 50.46 %\n",
      "Percentage of entangled states in the test set = 49.54 %\n"
     ]
    }
   ],
   "source": [
    "# Composition of the training and of the test datasets  \n",
    "\n",
    "train_sep = labels_werner_train.loc[labels_werner_train[\"State type\"] == 0].values\n",
    "train_ent = labels_werner_train.loc[labels_werner_train[\"State type\"] == 1].values\n",
    "\n",
    "test_sep = labels_werner_test.loc[labels_werner_test[\"State type\"] == 0].values\n",
    "test_ent = labels_werner_test.loc[labels_werner_test[\"State type\"] == 1].values\n",
    "\n",
    "train_sep_perc = train_sep.size/nrstates_train*100\n",
    "train_ent_perc = train_ent.size/nrstates_train*100\n",
    "\n",
    "test_sep_perc = test_sep.size/nrstates_test*100\n",
    "test_ent_perc = test_ent.size/nrstates_test*100\n",
    "\n",
    "print(\"Percentage of separable states in the training set = {} %\".format(round(train_sep_perc,2)))\n",
    "print(\"Percentage of entangled states in the training set = {} %\".format(round(train_ent_perc,2)))\n",
    "\n",
    "print(\"Percentage of separable states in the test set = {} %\".format(round(test_sep_perc,2)))\n",
    "print(\"Percentage of entangled states in the test set = {} %\".format(round(test_ent_perc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho(0,0)</th>\n",
       "      <th>rho(0,1)</th>\n",
       "      <th>rho(0,2)</th>\n",
       "      <th>rho(0,3)</th>\n",
       "      <th>rho(1,0)</th>\n",
       "      <th>rho(1,1)</th>\n",
       "      <th>rho(1,2)</th>\n",
       "      <th>rho(1,3)</th>\n",
       "      <th>rho(2,0)</th>\n",
       "      <th>rho(2,1)</th>\n",
       "      <th>rho(2,2)</th>\n",
       "      <th>rho(2,3)</th>\n",
       "      <th>rho(3,0)</th>\n",
       "      <th>rho(3,1)</th>\n",
       "      <th>rho(3,2)</th>\n",
       "      <th>rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(0.022271438846287326+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.47772856115371265+0j)</td>\n",
       "      <td>(-0.4554571223074253+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.4554571223074253+0j)</td>\n",
       "      <td>(0.47772856115371265+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.022271438846287326+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(0.2030728658783181+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.29692713412168187+0j)</td>\n",
       "      <td>(-0.09385426824336378+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.09385426824336378+0j)</td>\n",
       "      <td>(0.29692713412168187+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.2030728658783181+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(0.31125243396845764+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.18874756603154233+0j)</td>\n",
       "      <td>(0.12250486793691533+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.12250486793691533+0j)</td>\n",
       "      <td>(0.18874756603154233+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.31125243396845764+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(0.09291484312860752+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.40708515687139246+0j)</td>\n",
       "      <td>(-0.3141703137427849+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.3141703137427849+0j)</td>\n",
       "      <td>(0.40708515687139246+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.09291484312860752+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(0.012517580352686842+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.48748241964731315+0j)</td>\n",
       "      <td>(-0.4749648392946263+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.4749648392946263+0j)</td>\n",
       "      <td>(0.48748241964731315+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.012517580352686842+0j)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rho(0,0) rho(0,1) rho(0,2) rho(0,3) rho(1,0)  \\\n",
       "0  (0.022271438846287326+0j)       0j       0j       0j       0j   \n",
       "1    (0.2030728658783181+0j)       0j       0j       0j       0j   \n",
       "2   (0.31125243396845764+0j)       0j       0j       0j       0j   \n",
       "3   (0.09291484312860752+0j)       0j       0j       0j       0j   \n",
       "4  (0.012517580352686842+0j)       0j       0j       0j       0j   \n",
       "\n",
       "                   rho(1,1)                   rho(1,2) rho(1,3) rho(2,0)  \\\n",
       "0  (0.47772856115371265+0j)   (-0.4554571223074253+0j)       0j       0j   \n",
       "1  (0.29692713412168187+0j)  (-0.09385426824336378+0j)       0j       0j   \n",
       "2  (0.18874756603154233+0j)   (0.12250486793691533+0j)       0j       0j   \n",
       "3  (0.40708515687139246+0j)   (-0.3141703137427849+0j)       0j       0j   \n",
       "4  (0.48748241964731315+0j)   (-0.4749648392946263+0j)       0j       0j   \n",
       "\n",
       "                    rho(2,1)                  rho(2,2) rho(2,3) rho(3,0)  \\\n",
       "0   (-0.4554571223074253+0j)  (0.47772856115371265+0j)       0j       0j   \n",
       "1  (-0.09385426824336378+0j)  (0.29692713412168187+0j)       0j       0j   \n",
       "2   (0.12250486793691533+0j)  (0.18874756603154233+0j)       0j       0j   \n",
       "3   (-0.3141703137427849+0j)  (0.40708515687139246+0j)       0j       0j   \n",
       "4   (-0.4749648392946263+0j)  (0.48748241964731315+0j)       0j       0j   \n",
       "\n",
       "  rho(3,1) rho(3,2)                   rho(3,3)  \n",
       "0       0j       0j  (0.022271438846287326+0j)  \n",
       "1       0j       0j    (0.2030728658783181+0j)  \n",
       "2       0j       0j   (0.31125243396845764+0j)  \n",
       "3       0j       0j   (0.09291484312860752+0j)  \n",
       "4       0j       0j  (0.012517580352686842+0j)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the feature space of the training dataset\n",
    "werner_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho(0,0)</th>\n",
       "      <th>rho(0,1)</th>\n",
       "      <th>rho(0,2)</th>\n",
       "      <th>rho(0,3)</th>\n",
       "      <th>rho(1,0)</th>\n",
       "      <th>rho(1,1)</th>\n",
       "      <th>rho(1,2)</th>\n",
       "      <th>rho(1,3)</th>\n",
       "      <th>rho(2,0)</th>\n",
       "      <th>rho(2,1)</th>\n",
       "      <th>rho(2,2)</th>\n",
       "      <th>rho(2,3)</th>\n",
       "      <th>rho(3,0)</th>\n",
       "      <th>rho(3,1)</th>\n",
       "      <th>rho(3,2)</th>\n",
       "      <th>rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(0.15475194015036614+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.34524805984963386+0j)</td>\n",
       "      <td>(-0.19049611969926772+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.19049611969926772+0j)</td>\n",
       "      <td>(0.34524805984963386+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.15475194015036614+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(0.3082079319731107+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.19179206802688933+0j)</td>\n",
       "      <td>(0.11641586394622132+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.11641586394622132+0j)</td>\n",
       "      <td>(0.19179206802688933+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.3082079319731107+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(0.06464156095814991+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.4353584390418501+0j)</td>\n",
       "      <td>(-0.37071687808370024+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.37071687808370024+0j)</td>\n",
       "      <td>(0.4353584390418501+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.06464156095814991+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(0.1311521153345864+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.3688478846654136+0j)</td>\n",
       "      <td>(-0.23769576933082717+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.23769576933082717+0j)</td>\n",
       "      <td>(0.3688478846654136+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.1311521153345864+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(0.08890351092847841+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.4110964890715216+0j)</td>\n",
       "      <td>(-0.32219297814304315+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(-0.32219297814304315+0j)</td>\n",
       "      <td>(0.4110964890715216+0j)</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>0j</td>\n",
       "      <td>(0.08890351092847841+0j)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rho(0,0) rho(0,1) rho(0,2) rho(0,3) rho(1,0)  \\\n",
       "0  (0.15475194015036614+0j)       0j       0j       0j       0j   \n",
       "1   (0.3082079319731107+0j)       0j       0j       0j       0j   \n",
       "2  (0.06464156095814991+0j)       0j       0j       0j       0j   \n",
       "3   (0.1311521153345864+0j)       0j       0j       0j       0j   \n",
       "4  (0.08890351092847841+0j)       0j       0j       0j       0j   \n",
       "\n",
       "                   rho(1,1)                   rho(1,2) rho(1,3) rho(2,0)  \\\n",
       "0  (0.34524805984963386+0j)  (-0.19049611969926772+0j)       0j       0j   \n",
       "1  (0.19179206802688933+0j)   (0.11641586394622132+0j)       0j       0j   \n",
       "2   (0.4353584390418501+0j)  (-0.37071687808370024+0j)       0j       0j   \n",
       "3   (0.3688478846654136+0j)  (-0.23769576933082717+0j)       0j       0j   \n",
       "4   (0.4110964890715216+0j)  (-0.32219297814304315+0j)       0j       0j   \n",
       "\n",
       "                    rho(2,1)                  rho(2,2) rho(2,3) rho(3,0)  \\\n",
       "0  (-0.19049611969926772+0j)  (0.34524805984963386+0j)       0j       0j   \n",
       "1   (0.11641586394622132+0j)  (0.19179206802688933+0j)       0j       0j   \n",
       "2  (-0.37071687808370024+0j)   (0.4353584390418501+0j)       0j       0j   \n",
       "3  (-0.23769576933082717+0j)   (0.3688478846654136+0j)       0j       0j   \n",
       "4  (-0.32219297814304315+0j)   (0.4110964890715216+0j)       0j       0j   \n",
       "\n",
       "  rho(3,1) rho(3,2)                  rho(3,3)  \n",
       "0       0j       0j  (0.15475194015036614+0j)  \n",
       "1       0j       0j   (0.3082079319731107+0j)  \n",
       "2       0j       0j  (0.06464156095814991+0j)  \n",
       "3       0j       0j   (0.1311521153345864+0j)  \n",
       "4       0j       0j  (0.08890351092847841+0j)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the feature space of the test set\n",
    "werner_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each feature from a string into a complex number having real and imaginary part \n",
    "\n",
    "for col in werner_train.columns:\n",
    "    werner_train[col] = werner_train[col].apply(lambda x: np.complex(x)) # read features as complex numbers\n",
    "    werner_test[col] = werner_test[col].apply(lambda x: np.complex(x))\n",
    "\n",
    "for col in werner_train.columns:\n",
    "    werner_train[\"Re-\" + col] = werner_train[col].apply(lambda x: np.real(x)) # real parts of features\n",
    "    werner_train[\"Im-\" + col] = werner_train[col].apply(lambda x: np.imag(x)) # imaginary parts of features\n",
    "    werner_test[\"Re-\" + col] = werner_test[col].apply(lambda x: np.real(x))\n",
    "    werner_test[\"Im-\" + col] = werner_test[col].apply(lambda x: np.imag(x))\n",
    "    \n",
    "werner_train = werner_train.drop(columns = werner_train.columns[:16])\n",
    "werner_test = werner_test.drop(columns = werner_test.columns[:16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Im-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "      <th>Im-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.203073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311252</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Im-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  \\\n",
       "0     0.022271          0.0          0.0          0.0          0.0   \n",
       "1     0.203073          0.0          0.0          0.0          0.0   \n",
       "2     0.311252          0.0          0.0          0.0          0.0   \n",
       "3     0.092915          0.0          0.0          0.0          0.0   \n",
       "4     0.012518          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Im-rho(0,2)  Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  ...  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "3          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "   Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  Im-rho(3,0)  Re-rho(3,1)  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          0.0          0.0          0.0   \n",
       "3          0.0          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  Re-rho(3,3)  Im-rho(3,3)  \n",
       "0          0.0          0.0          0.0     0.022271          0.0  \n",
       "1          0.0          0.0          0.0     0.203073          0.0  \n",
       "2          0.0          0.0          0.0     0.311252          0.0  \n",
       "3          0.0          0.0          0.0     0.092915          0.0  \n",
       "4          0.0          0.0          0.0     0.012518          0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the feature space of the training set\n",
    "werner_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Im-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "      <th>Im-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.154752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154752</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.131152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088904</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Im-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  \\\n",
       "0     0.154752          0.0          0.0          0.0          0.0   \n",
       "1     0.308208          0.0          0.0          0.0          0.0   \n",
       "2     0.064642          0.0          0.0          0.0          0.0   \n",
       "3     0.131152          0.0          0.0          0.0          0.0   \n",
       "4     0.088904          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Im-rho(0,2)  Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  ...  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "3          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "   Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  Im-rho(3,0)  Re-rho(3,1)  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          0.0          0.0          0.0   \n",
       "3          0.0          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  Re-rho(3,3)  Im-rho(3,3)  \n",
       "0          0.0          0.0          0.0     0.154752          0.0  \n",
       "1          0.0          0.0          0.0     0.308208          0.0  \n",
       "2          0.0          0.0          0.0     0.064642          0.0  \n",
       "3          0.0          0.0          0.0     0.131152          0.0  \n",
       "4          0.0          0.0          0.0     0.088904          0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the feature space of the test set\n",
    "werner_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many columns are entirely composed of 0, therefore they should be dropped. One of the possible methods to select them is to check which columns have a zero variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with zero variance\n",
    "\n",
    "zero_var_cols = [] # empty array \n",
    "\n",
    "for item in werner_train.columns:\n",
    "    if werner_train[item].var() == 0:\n",
    "        zero_var_cols.append(item) # append zero-variance columns to the empty array \n",
    "    \n",
    "werner_train = werner_train.drop(columns = zero_var_cols) # drop columns with zero variance from the training set\n",
    "werner_test = werner_test.drop(columns = zero_var_cols) # drop columns with zero variance from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>Re-rho(1,2)</th>\n",
       "      <th>Re-rho(2,1)</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.477729</td>\n",
       "      <td>-0.455457</td>\n",
       "      <td>-0.455457</td>\n",
       "      <td>0.477729</td>\n",
       "      <td>0.022271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.203073</td>\n",
       "      <td>0.296927</td>\n",
       "      <td>-0.093854</td>\n",
       "      <td>-0.093854</td>\n",
       "      <td>0.296927</td>\n",
       "      <td>0.203073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311252</td>\n",
       "      <td>0.188748</td>\n",
       "      <td>0.122505</td>\n",
       "      <td>0.122505</td>\n",
       "      <td>0.188748</td>\n",
       "      <td>0.311252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092915</td>\n",
       "      <td>0.407085</td>\n",
       "      <td>-0.314170</td>\n",
       "      <td>-0.314170</td>\n",
       "      <td>0.407085</td>\n",
       "      <td>0.092915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>0.487482</td>\n",
       "      <td>-0.474965</td>\n",
       "      <td>-0.474965</td>\n",
       "      <td>0.487482</td>\n",
       "      <td>0.012518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Re-rho(1,1)  Re-rho(1,2)  Re-rho(2,1)  Re-rho(2,2)  \\\n",
       "0     0.022271     0.477729    -0.455457    -0.455457     0.477729   \n",
       "1     0.203073     0.296927    -0.093854    -0.093854     0.296927   \n",
       "2     0.311252     0.188748     0.122505     0.122505     0.188748   \n",
       "3     0.092915     0.407085    -0.314170    -0.314170     0.407085   \n",
       "4     0.012518     0.487482    -0.474965    -0.474965     0.487482   \n",
       "\n",
       "   Re-rho(3,3)  \n",
       "0     0.022271  \n",
       "1     0.203073  \n",
       "2     0.311252  \n",
       "3     0.092915  \n",
       "4     0.012518  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training set composed of non-zero features \n",
    "werner_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>Re-rho(1,2)</th>\n",
       "      <th>Re-rho(2,1)</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.154752</td>\n",
       "      <td>0.345248</td>\n",
       "      <td>-0.190496</td>\n",
       "      <td>-0.190496</td>\n",
       "      <td>0.345248</td>\n",
       "      <td>0.154752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.191792</td>\n",
       "      <td>0.116416</td>\n",
       "      <td>0.116416</td>\n",
       "      <td>0.191792</td>\n",
       "      <td>0.308208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>0.435358</td>\n",
       "      <td>-0.370717</td>\n",
       "      <td>-0.370717</td>\n",
       "      <td>0.435358</td>\n",
       "      <td>0.064642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.131152</td>\n",
       "      <td>0.368848</td>\n",
       "      <td>-0.237696</td>\n",
       "      <td>-0.237696</td>\n",
       "      <td>0.368848</td>\n",
       "      <td>0.131152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088904</td>\n",
       "      <td>0.411096</td>\n",
       "      <td>-0.322193</td>\n",
       "      <td>-0.322193</td>\n",
       "      <td>0.411096</td>\n",
       "      <td>0.088904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Re-rho(1,1)  Re-rho(1,2)  Re-rho(2,1)  Re-rho(2,2)  \\\n",
       "0     0.154752     0.345248    -0.190496    -0.190496     0.345248   \n",
       "1     0.308208     0.191792     0.116416     0.116416     0.191792   \n",
       "2     0.064642     0.435358    -0.370717    -0.370717     0.435358   \n",
       "3     0.131152     0.368848    -0.237696    -0.237696     0.368848   \n",
       "4     0.088904     0.411096    -0.322193    -0.322193     0.411096   \n",
       "\n",
       "   Re-rho(3,3)  \n",
       "0     0.154752  \n",
       "1     0.308208  \n",
       "2     0.064642  \n",
       "3     0.131152  \n",
       "4     0.088904  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set composed of non-zero features\n",
    "werner_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, it emerges that some components are equal. Let us check if this is true for both training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset, rho(1,1) = rho(2,2) for all rows\n",
      "Training dataset, rho(1,2) = rho(2,1) for all rows\n",
      "Training dataset, rho(0,0) = rho(3,3) for all rows\n",
      "Test dataset, rho(1,1) = rho(2,2) for all rows\n",
      "Test dataset, rho(1,2) = rho(2,1) for all rows\n",
      "Test dataset, rho(0,0) = rho(3,3) for all rows\n"
     ]
    }
   ],
   "source": [
    "# Training set \n",
    "\n",
    "if all(werner_train[[\"Re-rho(1,1)\"]].values == werner_train[[\"Re-rho(2,2)\"]].values):\n",
    "    print(\"Training dataset, rho(1,1) = rho(2,2) for all rows\")\n",
    "if all(werner_train[[\"Re-rho(1,2)\"]].values == werner_train[[\"Re-rho(2,1)\"]].values):\n",
    "    print(\"Training dataset, rho(1,2) = rho(2,1) for all rows\")\n",
    "if all(werner_train[[\"Re-rho(0,0)\"]].values == werner_train[[\"Re-rho(3,3)\"]].values):\n",
    "    print(\"Training dataset, rho(0,0) = rho(3,3) for all rows\")\n",
    "    \n",
    "# Test set\n",
    "\n",
    "if all(werner_test[[\"Re-rho(1,1)\"]].values == werner_test[[\"Re-rho(2,2)\"]].values):\n",
    "    print(\"Test dataset, rho(1,1) = rho(2,2) for all rows\")\n",
    "if all(werner_test[[\"Re-rho(1,2)\"]].values == werner_test[[\"Re-rho(2,1)\"]].values):\n",
    "    print(\"Test dataset, rho(1,2) = rho(2,1) for all rows\")\n",
    "if all(werner_test[[\"Re-rho(0,0)\"]].values == werner_test[[\"Re-rho(3,3)\"]].values):\n",
    "    print(\"Test dataset, rho(0,0) = rho(3,3) for all rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously stated that classifying entangled/separable Werner states is a linear task. We aim at checking the **correlation matrix** of both datasets, which may help us understand whether the states components are linearly correlated or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>Re-rho(1,2)</th>\n",
       "      <th>Re-rho(2,1)</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Re-rho(0,0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,1)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,2)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(3,3)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Re-rho(0,0)  Re-rho(1,1)  Re-rho(1,2)  Re-rho(2,1)  Re-rho(2,2)  \\\n",
       "Re-rho(0,0)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "Re-rho(1,1)         -1.0          1.0         -1.0         -1.0          1.0   \n",
       "Re-rho(1,2)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "Re-rho(2,1)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "Re-rho(2,2)         -1.0          1.0         -1.0         -1.0          1.0   \n",
       "Re-rho(3,3)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "\n",
       "             Re-rho(3,3)  \n",
       "Re-rho(0,0)          1.0  \n",
       "Re-rho(1,1)         -1.0  \n",
       "Re-rho(1,2)          1.0  \n",
       "Re-rho(2,1)          1.0  \n",
       "Re-rho(2,2)         -1.0  \n",
       "Re-rho(3,3)          1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix of the training dataset\n",
    "werner_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>Re-rho(1,2)</th>\n",
       "      <th>Re-rho(2,1)</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Re-rho(0,0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,1)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,2)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(3,3)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Re-rho(0,0)  Re-rho(1,1)  Re-rho(1,2)  Re-rho(2,1)  Re-rho(2,2)  \\\n",
       "Re-rho(0,0)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "Re-rho(1,1)         -1.0          1.0         -1.0         -1.0          1.0   \n",
       "Re-rho(1,2)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "Re-rho(2,1)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "Re-rho(2,2)         -1.0          1.0         -1.0         -1.0          1.0   \n",
       "Re-rho(3,3)          1.0         -1.0          1.0          1.0         -1.0   \n",
       "\n",
       "             Re-rho(3,3)  \n",
       "Re-rho(0,0)          1.0  \n",
       "Re-rho(1,1)         -1.0  \n",
       "Re-rho(1,2)          1.0  \n",
       "Re-rho(2,1)          1.0  \n",
       "Re-rho(2,2)         -1.0  \n",
       "Re-rho(3,3)          1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix of the test dataset\n",
    "werner_test.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are perfectly (anti-)correlated. This is related to the fact that they are linearly dependent on the parameter $p_{sym}$. Among others, this fact implies that a perceptron (i.e., a neural network with no hidden layers) should be able to classify them at highest accuracy. Let us check the validity of this statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting features and labels from both training and the test set into numpy arrays\n",
    "\n",
    "train_set = werner_train.values\n",
    "train_werner_labels = labels_werner_train.values\n",
    "\n",
    "test_set = werner_test.values\n",
    "test_werner_labels = labels_werner_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a perceptron in Keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "def perceptron(features_train, targets_train, features_test, targets_test): # build the model \n",
    "        \n",
    "    np.random.seed(0) # Fix a random seed realization \n",
    "    model = Sequential()  \n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # Output layer, made of 1 neuron and with the sigmoid act. function\n",
    "\n",
    "    # Tune the model hyperparameters\n",
    "    \n",
    "    # The optimizer is the standard Adam gradient descent with the binary cross entropy loss function, \n",
    "    # compatible with the sigmoid activation function on the output layer \n",
    "    \n",
    "    model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
    "    history = model.fit(features_train, targets_train, batch_size = 500, epochs = 1000, verbose = 0)\n",
    "    loss_train, acc_train = model.evaluate(features_train, targets_train)\n",
    "    loss_test, acc_test = model.evaluate(features_test, targets_test)\n",
    "    \n",
    "    # Show the trend loss function vs. # of epochs and accuracy vs. # of epochs\n",
    "    \n",
    "    plt.plot(history.history[\"binary_accuracy\"])\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.xlabel(\"Number of epochs\", fontsize = 25)\n",
    "    plt.yscale(\"log\") # logarithmic scale on both axes\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.legend([\"accuracy\",\"loss\"], loc = 7, fontsize = 14)\n",
    "\n",
    "    # The model output contains the accuracy on the training and on the test set. A comparison between them\n",
    "    # could help us understand whether there has been any overfitting. \n",
    "    # Along with this, we compute the model weights and the predicted classes\n",
    "    \n",
    "    return acc_train, acc_test, model.get_weights(), model.predict_classes(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marco/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/marco/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "20000/20000 [==============================] - 0s 12us/step\n",
      "5000/5000 [==============================] - 0s 12us/step\n",
      "Time to run the script: 37.53 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "net_output = perceptron(train_set, train_werner_labels, test_set, test_werner_labels)\n",
    "print(\"Time to run the script: {} seconds\".format(round(time.time()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set and on the test set respectively are 99.92 %, 99.92 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the training set and on the test set respectively are {} %, {} %\".format(round(net_output[0],4)*100, round(net_output[1],4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the **absolute value of the perceptron weights**, in order to understand whether there are components that affect the learning process more than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect the absolute value of the perceptron weights into a list\n",
    "\n",
    "n_net_weights = []\n",
    "\n",
    "for i in range(6):\n",
    "    n_net_weights.append(abs(net_output[2][0][i][0]))\n",
    "\n",
    "plt.bar([\"w$_{0,0}$\", \"w$_{1,1}$\", \"w$_{1,2}$\", \"w$_{2,1}$\", \"w$_{2,2}$\", \"w$_{3,3}$\"], n_net_weights)\n",
    "plt.title(\"Intensity of the model weights\", fontsize = 22)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron weights have the same order of magnitude, i.e. there is no preferential feature in the learning process. Moreover, weights of symmetric components are equal. \n",
    "\n",
    "We now check how many test samples have been misclassified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted boundary from the model is: 0.5003\n",
      "The number of misclassified targets is: 4\n"
     ]
    }
   ],
   "source": [
    "# Check the model predictions\n",
    "\n",
    "pred = net_output[3]\n",
    "\n",
    "predlabels = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    predlabels.append(pred[i][0])\n",
    "\n",
    "# Convert the numpy array of the test labels into a pandas dataframe\n",
    "    \n",
    "test_labels = pd.DataFrame(test_werner_labels)    \n",
    "    \n",
    "predlabels = np.asarray(predlabels)\n",
    "predlabels = pd.DataFrame(predlabels)\n",
    "\n",
    "# Build a dataset composed of: p_sym from the test set, labels from the test set, predicted labels from the model\n",
    "\n",
    "total = pd.concat([psym_test, test_labels, predlabels], axis = 1, ignore_index = True)\n",
    "total.columns = [\"psym\", \"actual_label\", \"pred_label\"]\n",
    "\n",
    "# Isolate the wrong predictions from the entire dataset\n",
    "\n",
    "wrong = total.loc[total[\"actual_label\"] != total[\"pred_label\"]]\n",
    "\n",
    "# Find the average prediction of p_sym from the model as the average of the misclassified values of p_sym\n",
    "\n",
    "psym_av = [[np.mean(wrong[\"psym\"].values)]]\n",
    "\n",
    "col = [\"Average psym\"]\n",
    "\n",
    "average_psym = pd.DataFrame(psym_av, columns = col)\n",
    "\n",
    "wrong_psyms = wrong[[\"psym\"]] # psym of the misclassified data\n",
    "\n",
    "# Print the average of the misclassified values of p_sym and the number of misclassified data\n",
    "\n",
    "print(\"The predicted boundary from the model is: {}\".format(round(average_psym.values[0][0], 4)))\n",
    "print(\"The number of misclassified targets is: {}\".format(wrong[\"psym\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>psym</th>\n",
       "      <th>actual_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.500221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.500503</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3345</td>\n",
       "      <td>0.500416</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3743</td>\n",
       "      <td>0.500251</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          psym  actual_label  pred_label\n",
       "188   0.500221             0           1\n",
       "1130  0.500503             0           1\n",
       "3345  0.500416             0           1\n",
       "3743  0.500251             0           1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset containing the wrong predicted labels of the test data\n",
    "wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we can see that the misclassified data have $p_{sym}$ close to the boundary at 0.5. Our model predicts this boundary at $p_{sym}=0.5003$, with an overestimation at the fourth decimal digit. Let us now represent $p_{sym}$ of all test data, highlighting the misclassified data with black triangles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Representation of the test data along with the entangled/separable boundary and the misclassified data\n",
    "\n",
    "boundary = [0.5]*5000 # Boundary as the line p_sym = 1/2\n",
    "\n",
    "ent_test = psym_test.loc[psym_test[\"p_sym\"] >= 1/2]\n",
    "sep_test = psym_test.loc[psym_test[\"p_sym\"] < 1/2]\n",
    "\n",
    "plt.plot(ent_test, \"bo\", marker = \"o\", c = \"b\", markersize = 2, label = \"Entangled\") # Entangled (1) targets\n",
    "plt.plot(sep_test, \"bo\", marker = \"o\", c = \"r\", markersize = 2, label = \"Separable\") # Separable (0) targets\n",
    "plt.plot(wrong_psyms, \"bo\", marker = \"v\", c = \"black\", markersize = 16) # Misclassified data (yellow triangles)\n",
    "plt.plot(boundary, c = \"y\", linewidth = 2) # Boundary between entangled and separable states\n",
    "plt.ylabel(\"p$_{sym}$\", fontsize = 20) \n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.legend(loc = 1, fontsize = 14, bbox_to_anchor=(0.9, 1.2), ncol=3, fancybox = True, shadow = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "Given that all features are perfectly (anti-)correlated, we could perform a dimensionality reduction through **principal component analysis** (PCA) of the previous datasets. All of the non-zero components are linearly dependent on $p_{sym}$. As a result, a dimensionality reduction to *one single component* is supposed to enclose all of the data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance enclosed by the principal component is 100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Check whether the PCA decomposition on the only one principal component yields all the data variance\n",
    "\n",
    "pca_1 = PCA(n_components = 1)\n",
    "pca_1.fit(train_set)\n",
    "\n",
    "print(\"The variance enclosed by the principal component is {} %\".format(round(pca_1.explained_variance_ratio_[0],3)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim. reduction on both training and test set\n",
    "\n",
    "comp_pca_train = pca_1.fit_transform(train_set)\n",
    "comp_pca_test = pca_1.fit_transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the previous perceptron on the reduced feature space\n",
    "\n",
    "def perceptron_pca(features_train, targets_train, features_test, targets_test): # build the model \n",
    "        \n",
    "    np.random.seed(0) # Fix a random seed realization \n",
    "    model = Sequential()  \n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # Output layer, made of 1 neuron and with the sigmoid activation function\n",
    "\n",
    "    # Tune the model hyperparameters\n",
    "    \n",
    "    # The optimizer is the standard Adam gradient descent with the binary cross entropy loss function, compatible with\n",
    "    # the sigmoid activation function on the output layer \n",
    "    \n",
    "    model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
    "    history = model.fit(features_train, targets_train, batch_size = 500, epochs = 1000, verbose = 0)\n",
    "    loss_train, acc_train = model.evaluate(features_train, targets_train)\n",
    "    loss_test, acc_test = model.evaluate(features_test, targets_test)\n",
    "    \n",
    "    # Show the trend loss function vs. # of epochs and accuracy vs. # of epochs\n",
    "    \n",
    "    plt.plot(history.history[\"binary_accuracy\"])\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.xlabel(\"Number of epochs\", fontsize = 25)\n",
    "    plt.yscale(\"log\") # logarithmic scale on both axes\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.legend([\"accuracy\",\"loss\"], loc = 7, fontsize = 14)\n",
    "\n",
    "    # The model output contains the accuracy on the training and on the test set. A comparison between them\n",
    "    # could help us understand whether there has been any overfitting. \n",
    "    # Along with this, we compute the model weights and the predicted classes\n",
    "    \n",
    "    return acc_train, acc_test, model.get_weights(), model.predict_classes(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s 15us/step\n",
      "5000/5000 [==============================] - 0s 12us/step\n",
      "Time to run the script: 34.68 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca_output = perceptron_pca(comp_pca_train, train_werner_labels, comp_pca_test, test_werner_labels)\n",
    "print(\"Time to run the script: {} seconds\".format(round(time.time()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the PCA-transformed training set and on the PCA-transformed test set is 99.92 %, 99.92 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the PCA-transformed training set and on the PCA-transformed test set is {} %, {} %\".format(round(net_output[0],4)*100, round(net_output[1],4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA-transformed features have identical classification accuracies (99.92 \\%) as before. We can conclude that the PCA dimensionality reduction has not affected the accuracy of the model, but it has mapped a six-dimensional problem into a one-dimensional, i.e. linear, one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aim at checking whether a convolutional neural network can outperform the previous perceptron model. First, we reshape the input in order to build a model with a one-dimensional convolution. We convert both training and test labels into categorical variables, which requires to use two output neurons in place of one. Further, the activation function of the output layer will evaluate the probability for each point to be entangled or separable based upon majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input \n",
    "\n",
    "train_dtset = train_set.reshape(20000,6,1)\n",
    "test_dtset = test_set.reshape(5000,6,1)\n",
    "\n",
    "# Convert targets to categorical variables \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tr_lab = to_categorical(train_werner_labels)\n",
    "ts_lab = to_categorical(test_werner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "\n",
    "def conv_nn(features_train, targets_train, features_test, targets_test):\n",
    "    \n",
    "    np.random.seed(0) # Fix a random seed realization \n",
    "    model = Sequential() \n",
    "    model.add(Conv1D(1, kernel_size = 1, activation = \"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation = \"softmax\")) # Output layer, made of 1 neuron and with the sigmoid activation function\n",
    "\n",
    "    # Tune the model hyperparameters\n",
    "    \n",
    "    # The optimizer is the standard Adam gradient descent with the binary cross entropy loss function, compatible with\n",
    "    # the sigmoid activation function on the output layer \n",
    "    \n",
    "    model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
    "    history = model.fit(features_train, targets_train, batch_size = 100, epochs = 600, verbose = 0)\n",
    "    loss_train, acc_train = model.evaluate(features_train, targets_train)\n",
    "    loss_test, acc_test = model.evaluate(features_test, targets_test)\n",
    "    \n",
    "    # Show the trend loss function vs. # of epochs and accuracy vs. # of epochs\n",
    "    \n",
    "    plt.plot(history.history[\"binary_accuracy\"])\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.xlabel(\"Number of epochs\", fontsize = 25)\n",
    "    plt.yscale(\"log\") # logarithmic scale on both axes\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.legend([\"accuracy\",\"loss\"], loc = 7, fontsize = 14)\n",
    "\n",
    "    # The model output contains the accuracy on the training and on the test set. A comparison between them\n",
    "    # could help us understand whether there has been any overfitting. \n",
    "    # Along with this, we compute the model weights and the predicted classes\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s 16us/step\n",
      "5000/5000 [==============================] - 0s 14us/step\n",
      "Time to run the script: 108.9 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "acc_conv = conv_nn(train_dtset, tr_lab, test_dtset, ts_lab)\n",
    "print(\"Time to run the script: {} seconds\".format(round(time.time()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set and on the test set respectively are: 99.9 %, 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the training set and on the test set respectively are: {} %, {} %\".format(round(acc_conv[0],3)*100, round(acc_conv[1],3)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous convolutional neural network outperforms indeed the single perceptron, leading to maximal accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General states\n",
    "Every state is a $4 \\times 4$ complex matrix, so the feature space of training and test sets is made of 16 complex numbers. Each of these numbers should be split into real and imaginary part, therefore we will end up with 32 features. Unlike Werner states, all real and imaginary components of general states are non-zero. Symmetric components are complex conjugate, therefore their real part is equal and their imaginary part is opposite. As a theoretical fact, each $d \\times d$ quantum state is identified by a number of independent components equal to $d^2-1$. For $4\\times 4$ states, these components are $16-1=15$. As we will see, this could be checked also retaining the first 15 principal components from the PCA.\n",
    "\n",
    "Here, the boundary between entangled and separable states is a highly non-linear function, therefore we would need a larger network than a simple perceptron. Let us show the code we used to sample uniformly general quantum states. In what follows, we are going to perform a similar analysis to the one made for Werner states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to sample general states describing systems made of pairs of two-level particles \n",
    "\n",
    "# Import Python libraries \n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "import random\n",
    "\n",
    "\n",
    "dimspace = 2 # dimensional parameter (the single particle is a qubit, i.e. a two-level system)\n",
    "\n",
    "nrstates_train = 20000 # training set size\n",
    "nrstates_test = 5000 # test set size\n",
    "\n",
    "# Pauli matrices \n",
    "\n",
    "X = qt.sigmax()\n",
    "Y = qt.sigmay()\n",
    "Z = qt.sigmaz()\n",
    "I = qt.qeye(dimspace)\n",
    "\n",
    "\n",
    "\n",
    "def sampling_purestates(numberOfStates, dimsingle):\n",
    "\n",
    "    # Creating empty numpy arrays\n",
    "    \n",
    "    states = []\n",
    "    labelsPur = []\n",
    "\n",
    "    totalDim = dimsingle**2\n",
    "\n",
    "    for k in range(numberOfStates): \n",
    "\n",
    "        prob = random.random() # parameter used to decide which sampling procedure to follow\n",
    "\n",
    "        if prob > 0.5:\n",
    "\n",
    "            psi = qt.rand_ket(dimsingle)\n",
    "            phi = qt.rand_ket(dimsingle)\n",
    "            psiPhi = qt.tensor(psi, phi)\n",
    "            rho = qt.ket2dm(psiPhi)\n",
    "\n",
    "        elif prob <= 0.5:\n",
    "\n",
    "            nbkets = int(prob * 10 + 1)\n",
    "\n",
    "            psi = qt.rand_ket(dimsingle)\n",
    "            phi = qt.rand_ket(dimsingle)\n",
    "            psiPhiEnt = qt.tensor(psi, phi)\n",
    "\n",
    "            for k in range(nbkets):\n",
    "                psi = qt.rand_ket(dimsingle)\n",
    "                phi = qt.rand_ket(dimsingle)\n",
    "                psiPhiEnt += qt.tensor(psi, phi)\n",
    "\n",
    "            rho = qt.ket2dm(psiPhiEnt.unit())\n",
    "\n",
    "\n",
    "\n",
    "        # convert rho from a quobj to an array\n",
    "        \n",
    "        \n",
    "        rhoFull = rho.full()\n",
    "        \n",
    "        # appending states to the lists\n",
    "        \n",
    "        states.append(rhoFull)\n",
    "        \n",
    "        \n",
    "        # PPT criterion (analytical criterion to label the states)\n",
    "        \n",
    "        rhoPTA = rho.ptrace(1)\n",
    "        rhoPTB = rho.ptrace(0)\n",
    "\n",
    "        rhoPTA2 = rhoPTA*rhoPTA\n",
    "        rhoPTB2 = rhoPTB*rhoPTB\n",
    "\n",
    "        trPTA2 = rhoPTA2.tr()\n",
    "        trPTB2 = rhoPTB2.tr()\n",
    "\n",
    "\n",
    "        if round(trPTA2,14) == 1 and round(trPTB2,14) == 1:\n",
    "            labelsPur.append(0)\n",
    "        else:\n",
    "            labelsPur.append(1)\n",
    "\n",
    "\n",
    "    # Data Frames created from the arrays\n",
    "    # we use first array and then create from them data frames to have more\n",
    "    # efficient code. We should remember to reshape our numpy arrays\n",
    "\n",
    "    # To make the headers of the pandas data frames\n",
    "    dmElementsList = []\n",
    "    for m in range(totalDim):\n",
    "        for n in range(totalDim):\n",
    "            dmElementsList.append(\"rho(\" + str(m) + \",\" + str(n) + \")\")\n",
    "\n",
    "    # Pandas data frames\n",
    "\n",
    "    dfDM = pd.DataFrame(np.reshape(states, (numberOfStates, totalDim**2)), columns = dmElementsList)\n",
    "    dfLabelsPUR = pd.DataFrame(np.reshape(labelsPur, (numberOfStates, 1)), columns = [\"Labels\"])\n",
    "\n",
    "\n",
    "    return dfDM, dfLabelsPUR\n",
    "\n",
    "\n",
    "\n",
    "pure_train, labels_train = sampling_purestates(nrstates_train, dimspace)\n",
    "\n",
    "pure_states, labels_pure = sampling_purestates(nrstates_test, dimspace)\n",
    "\n",
    "\n",
    "pure_train.to_csv(\"train_set.csv\")\n",
    "labels_train.to_csv(\"train_labels.csv\")\n",
    "\n",
    "\n",
    "pure_states.to_csv(\"test_set.csv\")\n",
    "labels_pure.to_csv(\"test_labels.csv\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "\n",
    "general_train = pd.read_csv(\"train_set.csv\") # feature space of the training set\n",
    "labels_general_train = pd.read_csv(\"train_labels.csv\") # labels of the training set\n",
    "\n",
    "general_test = pd.read_csv(\"test_set.csv\") # features space of the test set\n",
    "labels_general_test = pd.read_csv(\"test_labels.csv\") # labels of the test set\n",
    "\n",
    "# Drop additional columns, added while importing the datasets\n",
    "\n",
    "labels_general_train = labels_general_train.drop(columns = [\"Unnamed: 0\"])\n",
    "labels_general_test = labels_general_test.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "general_train = general_train.drop(columns = [\"Unnamed: 0\"])\n",
    "general_test = general_test.drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Labels\n",
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the targets of the training set\n",
    "labels_general_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Labels\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the targets of the test set\n",
    "labels_general_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of separable states in the training set = 49.7 %\n",
      "Percentage of entangled states in the training set = 50.3 %\n",
      "Percentage of separable states in the test set = 49.72 %\n",
      "Percentage of entangled states in the test set = 50.28 %\n"
     ]
    }
   ],
   "source": [
    "# Check the composition of the training and of the test datasets to see whether we have balanced classes or not\n",
    "\n",
    "train_sep = labels_general_train.loc[labels_general_train[\"Labels\"] == 0].values\n",
    "train_ent = labels_general_train.loc[labels_general_train[\"Labels\"] == 1].values\n",
    "\n",
    "test_sep = labels_general_test.loc[labels_general_test[\"Labels\"] == 0].values\n",
    "test_ent = labels_general_test.loc[labels_general_test[\"Labels\"] == 1].values\n",
    "\n",
    "train_sep_perc = train_sep.size/nrstates_train*100\n",
    "train_ent_perc = train_ent.size/nrstates_train*100\n",
    "\n",
    "test_sep_perc = test_sep.size/nrstates_test*100\n",
    "test_ent_perc = test_ent.size/nrstates_test*100\n",
    "\n",
    "print(\"Percentage of separable states in the training set = {} %\".format(round(train_sep_perc,2)))\n",
    "print(\"Percentage of entangled states in the training set = {} %\".format(round(train_ent_perc,2)))\n",
    "\n",
    "print(\"Percentage of separable states in the test set = {} %\".format(round(test_sep_perc,2)))\n",
    "print(\"Percentage of entangled states in the test set = {} %\".format(round(test_ent_perc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho(0,0)</th>\n",
       "      <th>rho(0,1)</th>\n",
       "      <th>rho(0,2)</th>\n",
       "      <th>rho(0,3)</th>\n",
       "      <th>rho(1,0)</th>\n",
       "      <th>rho(1,1)</th>\n",
       "      <th>rho(1,2)</th>\n",
       "      <th>rho(1,3)</th>\n",
       "      <th>rho(2,0)</th>\n",
       "      <th>rho(2,1)</th>\n",
       "      <th>rho(2,2)</th>\n",
       "      <th>rho(2,3)</th>\n",
       "      <th>rho(3,0)</th>\n",
       "      <th>rho(3,1)</th>\n",
       "      <th>rho(3,2)</th>\n",
       "      <th>rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(0.05347348350690947+0j)</td>\n",
       "      <td>(0.02037055098459143-0.02039645187856153j)</td>\n",
       "      <td>(0.19103952630683146-0.006027367805933173j)</td>\n",
       "      <td>(0.056842648516309234-0.10009784716636119j)</td>\n",
       "      <td>(0.02037055098459143+0.02039645187856153j)</td>\n",
       "      <td>(0.01553993759435819+0j)</td>\n",
       "      <td>(0.07507491685285261+0.07057231836297355j)</td>\n",
       "      <td>(0.05983445967501923-0.01645039552965817j)</td>\n",
       "      <td>(0.19103952630683146+0.006027367805933173j)</td>\n",
       "      <td>(0.07507491685285261-0.07057231836297355j)</td>\n",
       "      <td>(0.683187766689748+0j)</td>\n",
       "      <td>(0.21435893897868227-0.35120273686485104j)</td>\n",
       "      <td>(0.056842648516309234+0.10009784716636119j)</td>\n",
       "      <td>(0.05983445967501923+0.01645039552965817j)</td>\n",
       "      <td>(0.21435893897868227+0.35120273686485104j)</td>\n",
       "      <td>(0.24779881220898425+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(0.060807688150406054+0j)</td>\n",
       "      <td>(0.08797496720788356+0.026243395929988707j)</td>\n",
       "      <td>(-0.10443585786683318-0.16160933286883744j)</td>\n",
       "      <td>(0.09744316696468948-0.04649953612932324j)</td>\n",
       "      <td>(0.08797496720788356-0.026243395929988707j)</td>\n",
       "      <td>(0.13860600429865302+0j)</td>\n",
       "      <td>(-0.22084245081648285-0.1887396896910304j)</td>\n",
       "      <td>(0.1209099359765602-0.1093288526451068j)</td>\n",
       "      <td>(-0.10443585786683318+0.16160933286883744j)</td>\n",
       "      <td>(-0.22084245081648285+0.1887396896910304j)</td>\n",
       "      <td>(0.6088773641108217+0j)</td>\n",
       "      <td>(-0.04377409836835404+0.33883781440436656j)</td>\n",
       "      <td>(0.09744316696468948+0.04649953612932324j)</td>\n",
       "      <td>(0.1209099359765602+0.1093288526451068j)</td>\n",
       "      <td>(-0.04377409836835404-0.33883781440436656j)</td>\n",
       "      <td>(0.19170894344011902+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(0.005376008547605305+0j)</td>\n",
       "      <td>(-0.0020197609023759598+0.005152857965634789j)</td>\n",
       "      <td>(-0.05076655557538025-0.0019368388157479655j)</td>\n",
       "      <td>(0.020929386258034588-0.0479316386289916j)</td>\n",
       "      <td>(-0.0020197609023759598-0.005152857965634789j)</td>\n",
       "      <td>(0.005697792152956478+0j)</td>\n",
       "      <td>(0.017216499557241225+0.04938697534566942j)</td>\n",
       "      <td>(-0.05380521244127814-0.002052769244726386j)</td>\n",
       "      <td>(-0.05076655557538025+0.0019368388157479655j)</td>\n",
       "      <td>(0.017216499557241225-0.04938697534566942j)</td>\n",
       "      <td>(0.48009494157817895+0j)</td>\n",
       "      <td>(-0.18037117758304366+0.46016687326028305j)</td>\n",
       "      <td>(0.020929386258034588+0.0479316386289916j)</td>\n",
       "      <td>(-0.05380521244127814+0.002052769244726386j)</td>\n",
       "      <td>(-0.18037117758304366-0.46016687326028305j)</td>\n",
       "      <td>(0.5088312577212593+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(0.2937530508943436+0j)</td>\n",
       "      <td>(-0.04424486747467146-0.2302469575787628j)</td>\n",
       "      <td>(-0.20345100776821812-0.22750324435921357j)</td>\n",
       "      <td>(-0.1476759708355046+0.1937335672886878j)</td>\n",
       "      <td>(-0.04424486747467146+0.2302469575787628j)</td>\n",
       "      <td>(0.18713429394100073+0j)</td>\n",
       "      <td>(0.20896325175999012-0.1252008261574193j)</td>\n",
       "      <td>(-0.1296077115603624-0.14493009986732416j)</td>\n",
       "      <td>(-0.20345100776821812+0.22750324435921357j)</td>\n",
       "      <td>(0.20896325175999012+0.1252008261574193j)</td>\n",
       "      <td>(0.31710322147215947+0j)</td>\n",
       "      <td>(-0.047761852913907034-0.24854908488644128j)</td>\n",
       "      <td>(-0.1476759708355046-0.1937335672886878j)</td>\n",
       "      <td>(-0.1296077115603624+0.14493009986732416j)</td>\n",
       "      <td>(-0.047761852913907034+0.24854908488644128j)</td>\n",
       "      <td>(0.20200943369249608+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(0.05764777899367143+0j)</td>\n",
       "      <td>(0.07088497983829455-0.019271735537371402j)</td>\n",
       "      <td>(-0.05877411809792195-0.12326412137399771j)</td>\n",
       "      <td>(-0.11347732452049927-0.13192000855384903j)</td>\n",
       "      <td>(0.07088497983829455+0.019271735537371402j)</td>\n",
       "      <td>(0.09360430274148447+0j)</td>\n",
       "      <td>(-0.031062577939302546-0.17121655319782336j)</td>\n",
       "      <td>(-0.09543317088426953-0.20014738357086428j)</td>\n",
       "      <td>(-0.05877411809792195+0.12326412137399771j)</td>\n",
       "      <td>(-0.031062577939302546+0.17121655319782336j)</td>\n",
       "      <td>(0.3234893156653848+0j)</td>\n",
       "      <td>(0.39776959354083385-0.10814294409075056j)</td>\n",
       "      <td>(-0.11347732452049927+0.13192000855384903j)</td>\n",
       "      <td>(-0.09543317088426953+0.20014738357086428j)</td>\n",
       "      <td>(0.39776959354083385+0.10814294409075056j)</td>\n",
       "      <td>(0.5252586025994592+0j)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rho(0,0)                                        rho(0,1)  \\\n",
       "0   (0.05347348350690947+0j)      (0.02037055098459143-0.02039645187856153j)   \n",
       "1  (0.060807688150406054+0j)     (0.08797496720788356+0.026243395929988707j)   \n",
       "2  (0.005376008547605305+0j)  (-0.0020197609023759598+0.005152857965634789j)   \n",
       "3    (0.2937530508943436+0j)      (-0.04424486747467146-0.2302469575787628j)   \n",
       "4   (0.05764777899367143+0j)     (0.07088497983829455-0.019271735537371402j)   \n",
       "\n",
       "                                        rho(0,2)  \\\n",
       "0    (0.19103952630683146-0.006027367805933173j)   \n",
       "1    (-0.10443585786683318-0.16160933286883744j)   \n",
       "2  (-0.05076655557538025-0.0019368388157479655j)   \n",
       "3    (-0.20345100776821812-0.22750324435921357j)   \n",
       "4    (-0.05877411809792195-0.12326412137399771j)   \n",
       "\n",
       "                                      rho(0,3)  \\\n",
       "0  (0.056842648516309234-0.10009784716636119j)   \n",
       "1   (0.09744316696468948-0.04649953612932324j)   \n",
       "2   (0.020929386258034588-0.0479316386289916j)   \n",
       "3    (-0.1476759708355046+0.1937335672886878j)   \n",
       "4  (-0.11347732452049927-0.13192000855384903j)   \n",
       "\n",
       "                                         rho(1,0)                   rho(1,1)  \\\n",
       "0      (0.02037055098459143+0.02039645187856153j)   (0.01553993759435819+0j)   \n",
       "1     (0.08797496720788356-0.026243395929988707j)   (0.13860600429865302+0j)   \n",
       "2  (-0.0020197609023759598-0.005152857965634789j)  (0.005697792152956478+0j)   \n",
       "3      (-0.04424486747467146+0.2302469575787628j)   (0.18713429394100073+0j)   \n",
       "4     (0.07088497983829455+0.019271735537371402j)   (0.09360430274148447+0j)   \n",
       "\n",
       "                                       rho(1,2)  \\\n",
       "0    (0.07507491685285261+0.07057231836297355j)   \n",
       "1    (-0.22084245081648285-0.1887396896910304j)   \n",
       "2   (0.017216499557241225+0.04938697534566942j)   \n",
       "3     (0.20896325175999012-0.1252008261574193j)   \n",
       "4  (-0.031062577939302546-0.17121655319782336j)   \n",
       "\n",
       "                                       rho(1,3)  \\\n",
       "0    (0.05983445967501923-0.01645039552965817j)   \n",
       "1      (0.1209099359765602-0.1093288526451068j)   \n",
       "2  (-0.05380521244127814-0.002052769244726386j)   \n",
       "3    (-0.1296077115603624-0.14493009986732416j)   \n",
       "4   (-0.09543317088426953-0.20014738357086428j)   \n",
       "\n",
       "                                        rho(2,0)  \\\n",
       "0    (0.19103952630683146+0.006027367805933173j)   \n",
       "1    (-0.10443585786683318+0.16160933286883744j)   \n",
       "2  (-0.05076655557538025+0.0019368388157479655j)   \n",
       "3    (-0.20345100776821812+0.22750324435921357j)   \n",
       "4    (-0.05877411809792195+0.12326412137399771j)   \n",
       "\n",
       "                                       rho(2,1)                  rho(2,2)  \\\n",
       "0    (0.07507491685285261-0.07057231836297355j)    (0.683187766689748+0j)   \n",
       "1    (-0.22084245081648285+0.1887396896910304j)   (0.6088773641108217+0j)   \n",
       "2   (0.017216499557241225-0.04938697534566942j)  (0.48009494157817895+0j)   \n",
       "3     (0.20896325175999012+0.1252008261574193j)  (0.31710322147215947+0j)   \n",
       "4  (-0.031062577939302546+0.17121655319782336j)   (0.3234893156653848+0j)   \n",
       "\n",
       "                                       rho(2,3)  \\\n",
       "0    (0.21435893897868227-0.35120273686485104j)   \n",
       "1   (-0.04377409836835404+0.33883781440436656j)   \n",
       "2   (-0.18037117758304366+0.46016687326028305j)   \n",
       "3  (-0.047761852913907034-0.24854908488644128j)   \n",
       "4    (0.39776959354083385-0.10814294409075056j)   \n",
       "\n",
       "                                      rho(3,0)  \\\n",
       "0  (0.056842648516309234+0.10009784716636119j)   \n",
       "1   (0.09744316696468948+0.04649953612932324j)   \n",
       "2   (0.020929386258034588+0.0479316386289916j)   \n",
       "3    (-0.1476759708355046-0.1937335672886878j)   \n",
       "4  (-0.11347732452049927+0.13192000855384903j)   \n",
       "\n",
       "                                       rho(3,1)  \\\n",
       "0    (0.05983445967501923+0.01645039552965817j)   \n",
       "1      (0.1209099359765602+0.1093288526451068j)   \n",
       "2  (-0.05380521244127814+0.002052769244726386j)   \n",
       "3    (-0.1296077115603624+0.14493009986732416j)   \n",
       "4   (-0.09543317088426953+0.20014738357086428j)   \n",
       "\n",
       "                                       rho(3,2)                  rho(3,3)  \n",
       "0    (0.21435893897868227+0.35120273686485104j)  (0.24779881220898425+0j)  \n",
       "1   (-0.04377409836835404-0.33883781440436656j)  (0.19170894344011902+0j)  \n",
       "2   (-0.18037117758304366-0.46016687326028305j)   (0.5088312577212593+0j)  \n",
       "3  (-0.047761852913907034+0.24854908488644128j)  (0.20200943369249608+0j)  \n",
       "4    (0.39776959354083385+0.10814294409075056j)   (0.5252586025994592+0j)  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the feature space of the training set \n",
    "general_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho(0,0)</th>\n",
       "      <th>rho(0,1)</th>\n",
       "      <th>rho(0,2)</th>\n",
       "      <th>rho(0,3)</th>\n",
       "      <th>rho(1,0)</th>\n",
       "      <th>rho(1,1)</th>\n",
       "      <th>rho(1,2)</th>\n",
       "      <th>rho(1,3)</th>\n",
       "      <th>rho(2,0)</th>\n",
       "      <th>rho(2,1)</th>\n",
       "      <th>rho(2,2)</th>\n",
       "      <th>rho(2,3)</th>\n",
       "      <th>rho(3,0)</th>\n",
       "      <th>rho(3,1)</th>\n",
       "      <th>rho(3,2)</th>\n",
       "      <th>rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(0.0654804787485705+0j)</td>\n",
       "      <td>(-0.031046696237585783+0.02308405226435933j)</td>\n",
       "      <td>(-0.008521628963953311+0.2101823779446633j)</td>\n",
       "      <td>(-0.07005588015388181-0.10265933144341187j)</td>\n",
       "      <td>(-0.031046696237585783-0.02308405226435933j)</td>\n",
       "      <td>(0.022858275394714824+0j)</td>\n",
       "      <td>(0.07813671373816733-0.09665101470105235j)</td>\n",
       "      <td>(-0.0029747757712274767+0.07337158753256721j)</td>\n",
       "      <td>(-0.008521628963953311-0.2101823779446633j)</td>\n",
       "      <td>(0.07813671373816733+0.09665101470105235j)</td>\n",
       "      <td>(0.6757624715692624+0j)</td>\n",
       "      <td>(-0.32040376894814776+0.23822880513588227j)</td>\n",
       "      <td>(-0.07005588015388181+0.10265933144341187j)</td>\n",
       "      <td>(-0.0029747757712274767-0.07337158753256721j)</td>\n",
       "      <td>(-0.32040376894814776-0.23822880513588227j)</td>\n",
       "      <td>(0.2358987742874522+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(0.2865752420148217+0j)</td>\n",
       "      <td>(-0.350980923162389-0.06688504631230408j)</td>\n",
       "      <td>(-0.16138575568448219-0.06336537635385366j)</td>\n",
       "      <td>(0.18286689741560033+0.11527280514554011j)</td>\n",
       "      <td>(-0.350980923162389+0.06688504631230408j)</td>\n",
       "      <td>(0.4454719010148098+0j)</td>\n",
       "      <td>(0.21244514082718488+0.03993975357268653j)</td>\n",
       "      <td>(-0.25086891273657064-0.09849941838803412j)</td>\n",
       "      <td>(-0.16138575568448219+0.06336537635385366j)</td>\n",
       "      <td>(0.21244514082718488-0.03993975357268653j)</td>\n",
       "      <td>(0.10489577831975504+0j)</td>\n",
       "      <td>(-0.12847033418400036-0.024482083454086825j)</td>\n",
       "      <td>(0.18286689741560033-0.11527280514554011j)</td>\n",
       "      <td>(-0.25086891273657064+0.09849941838803412j)</td>\n",
       "      <td>(-0.12847033418400036+0.024482083454086825j)</td>\n",
       "      <td>(0.16305707865061336+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(0.4322363703387836+0j)</td>\n",
       "      <td>(-0.15785350189592515+0.36267566718833144j)</td>\n",
       "      <td>(-0.1804703880245037+0.1258757448708042j)</td>\n",
       "      <td>(-0.039710186817943166-0.1973969600227384j)</td>\n",
       "      <td>(-0.15785350189592515-0.36267566718833144j)</td>\n",
       "      <td>(0.361957897038333+0j)</td>\n",
       "      <td>(0.1715264091124981+0.10545686198776472j)</td>\n",
       "      <td>(-0.15112722253299019+0.10540926915950893j)</td>\n",
       "      <td>(-0.1804703880245037-0.1258757448708042j)</td>\n",
       "      <td>(0.1715264091124981-0.10545686198776472j)</td>\n",
       "      <td>(0.11200876979081602+0j)</td>\n",
       "      <td>(-0.040905804712075944+0.09398296418922022j)</td>\n",
       "      <td>(-0.039710186817943166+0.1973969600227384j)</td>\n",
       "      <td>(-0.15112722253299019-0.10540926915950893j)</td>\n",
       "      <td>(-0.040905804712075944-0.09398296418922022j)</td>\n",
       "      <td>(0.09379696283206723+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(0.2890269646048582+0j)</td>\n",
       "      <td>(-0.09246102469810215-0.30051235996820597j)</td>\n",
       "      <td>(-0.2122835373421186+0.061434353979974876j)</td>\n",
       "      <td>(0.13178609870850183+0.20106616539350947j)</td>\n",
       "      <td>(-0.09246102469810215+0.30051235996820597j)</td>\n",
       "      <td>(0.34203286090290963+0j)</td>\n",
       "      <td>(0.0040348162433219426-0.24037241717122976j)</td>\n",
       "      <td>(-0.25121512693108083+0.0727010639931817j)</td>\n",
       "      <td>(-0.2122835373421186-0.061434353979974876j)</td>\n",
       "      <td>(0.0040348162433219426+0.24037241717122976j)</td>\n",
       "      <td>(0.16897551459321036+0j)</td>\n",
       "      <td>(-0.05405602639718109-0.1756902880555461j)</td>\n",
       "      <td>(0.13178609870850183-0.20106616539350947j)</td>\n",
       "      <td>(-0.25121512693108083-0.0727010639931817j)</td>\n",
       "      <td>(-0.05405602639718109+0.1756902880555461j)</td>\n",
       "      <td>(0.19996465989902182+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(0.35438359303486977+0j)</td>\n",
       "      <td>(-0.26311323157290606-0.0928995223159682j)</td>\n",
       "      <td>(0.11303635494878088+0.28354235830007124j)</td>\n",
       "      <td>(-0.009595283360201948-0.24014872932009743j)</td>\n",
       "      <td>(-0.26311323157290606+0.0928995223159682j)</td>\n",
       "      <td>(0.21970230960328846+0j)</td>\n",
       "      <td>(-0.15825312283206422-0.1808851314252213j)</td>\n",
       "      <td>(0.07007759032721551+0.17578384612957876j)</td>\n",
       "      <td>(0.11303635494878088-0.28354235830007124j)</td>\n",
       "      <td>(-0.15825312283206422+0.1808851314252213j)</td>\n",
       "      <td>(0.2629170433443427+0j)</td>\n",
       "      <td>(-0.1952035993469853-0.06892211777144222j)</td>\n",
       "      <td>(-0.009595283360201948+0.24014872932009743j)</td>\n",
       "      <td>(0.07007759032721551-0.17578384612957876j)</td>\n",
       "      <td>(-0.1952035993469853+0.06892211777144222j)</td>\n",
       "      <td>(0.16299705401749887+0j)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rho(0,0)                                      rho(0,1)  \\\n",
       "0   (0.0654804787485705+0j)  (-0.031046696237585783+0.02308405226435933j)   \n",
       "1   (0.2865752420148217+0j)     (-0.350980923162389-0.06688504631230408j)   \n",
       "2   (0.4322363703387836+0j)   (-0.15785350189592515+0.36267566718833144j)   \n",
       "3   (0.2890269646048582+0j)   (-0.09246102469810215-0.30051235996820597j)   \n",
       "4  (0.35438359303486977+0j)    (-0.26311323157290606-0.0928995223159682j)   \n",
       "\n",
       "                                      rho(0,2)  \\\n",
       "0  (-0.008521628963953311+0.2101823779446633j)   \n",
       "1  (-0.16138575568448219-0.06336537635385366j)   \n",
       "2    (-0.1804703880245037+0.1258757448708042j)   \n",
       "3  (-0.2122835373421186+0.061434353979974876j)   \n",
       "4   (0.11303635494878088+0.28354235830007124j)   \n",
       "\n",
       "                                       rho(0,3)  \\\n",
       "0   (-0.07005588015388181-0.10265933144341187j)   \n",
       "1    (0.18286689741560033+0.11527280514554011j)   \n",
       "2   (-0.039710186817943166-0.1973969600227384j)   \n",
       "3    (0.13178609870850183+0.20106616539350947j)   \n",
       "4  (-0.009595283360201948-0.24014872932009743j)   \n",
       "\n",
       "                                       rho(1,0)                   rho(1,1)  \\\n",
       "0  (-0.031046696237585783-0.02308405226435933j)  (0.022858275394714824+0j)   \n",
       "1     (-0.350980923162389+0.06688504631230408j)    (0.4454719010148098+0j)   \n",
       "2   (-0.15785350189592515-0.36267566718833144j)     (0.361957897038333+0j)   \n",
       "3   (-0.09246102469810215+0.30051235996820597j)   (0.34203286090290963+0j)   \n",
       "4    (-0.26311323157290606+0.0928995223159682j)   (0.21970230960328846+0j)   \n",
       "\n",
       "                                       rho(1,2)  \\\n",
       "0    (0.07813671373816733-0.09665101470105235j)   \n",
       "1    (0.21244514082718488+0.03993975357268653j)   \n",
       "2     (0.1715264091124981+0.10545686198776472j)   \n",
       "3  (0.0040348162433219426-0.24037241717122976j)   \n",
       "4    (-0.15825312283206422-0.1808851314252213j)   \n",
       "\n",
       "                                        rho(1,3)  \\\n",
       "0  (-0.0029747757712274767+0.07337158753256721j)   \n",
       "1    (-0.25086891273657064-0.09849941838803412j)   \n",
       "2    (-0.15112722253299019+0.10540926915950893j)   \n",
       "3     (-0.25121512693108083+0.0727010639931817j)   \n",
       "4     (0.07007759032721551+0.17578384612957876j)   \n",
       "\n",
       "                                      rho(2,0)  \\\n",
       "0  (-0.008521628963953311-0.2101823779446633j)   \n",
       "1  (-0.16138575568448219+0.06336537635385366j)   \n",
       "2    (-0.1804703880245037-0.1258757448708042j)   \n",
       "3  (-0.2122835373421186-0.061434353979974876j)   \n",
       "4   (0.11303635494878088-0.28354235830007124j)   \n",
       "\n",
       "                                       rho(2,1)                  rho(2,2)  \\\n",
       "0    (0.07813671373816733+0.09665101470105235j)   (0.6757624715692624+0j)   \n",
       "1    (0.21244514082718488-0.03993975357268653j)  (0.10489577831975504+0j)   \n",
       "2     (0.1715264091124981-0.10545686198776472j)  (0.11200876979081602+0j)   \n",
       "3  (0.0040348162433219426+0.24037241717122976j)  (0.16897551459321036+0j)   \n",
       "4    (-0.15825312283206422+0.1808851314252213j)   (0.2629170433443427+0j)   \n",
       "\n",
       "                                       rho(2,3)  \\\n",
       "0   (-0.32040376894814776+0.23822880513588227j)   \n",
       "1  (-0.12847033418400036-0.024482083454086825j)   \n",
       "2  (-0.040905804712075944+0.09398296418922022j)   \n",
       "3    (-0.05405602639718109-0.1756902880555461j)   \n",
       "4    (-0.1952035993469853-0.06892211777144222j)   \n",
       "\n",
       "                                       rho(3,0)  \\\n",
       "0   (-0.07005588015388181+0.10265933144341187j)   \n",
       "1    (0.18286689741560033-0.11527280514554011j)   \n",
       "2   (-0.039710186817943166+0.1973969600227384j)   \n",
       "3    (0.13178609870850183-0.20106616539350947j)   \n",
       "4  (-0.009595283360201948+0.24014872932009743j)   \n",
       "\n",
       "                                        rho(3,1)  \\\n",
       "0  (-0.0029747757712274767-0.07337158753256721j)   \n",
       "1    (-0.25086891273657064+0.09849941838803412j)   \n",
       "2    (-0.15112722253299019-0.10540926915950893j)   \n",
       "3     (-0.25121512693108083-0.0727010639931817j)   \n",
       "4     (0.07007759032721551-0.17578384612957876j)   \n",
       "\n",
       "                                       rho(3,2)                  rho(3,3)  \n",
       "0   (-0.32040376894814776-0.23822880513588227j)   (0.2358987742874522+0j)  \n",
       "1  (-0.12847033418400036+0.024482083454086825j)  (0.16305707865061336+0j)  \n",
       "2  (-0.040905804712075944-0.09398296418922022j)  (0.09379696283206723+0j)  \n",
       "3    (-0.05405602639718109+0.1756902880555461j)  (0.19996465989902182+0j)  \n",
       "4    (-0.1952035993469853+0.06892211777144222j)  (0.16299705401749887+0j)  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the feature space of the test set\n",
    "general_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into real and imaginary part\n",
    "\n",
    "for col in general_train.columns:\n",
    "    general_train[col] = general_train[col].apply(lambda x: np.complex(x)) # read features as complex numbers\n",
    "    general_test[col] = general_test[col].apply(lambda x: np.complex(x))\n",
    "\n",
    "for col in general_train.columns:\n",
    "    general_train[\"Re-\" + col] = general_train[col].apply(lambda x: np.real(x)) # real parts of features\n",
    "    general_train[\"Im-\" + col] = general_train[col].apply(lambda x: np.imag(x)) # imaginary parts of features\n",
    "    general_test[\"Re-\" + col] = general_test[col].apply(lambda x: np.real(x))\n",
    "    general_test[\"Im-\" + col] = general_test[col].apply(lambda x: np.imag(x))\n",
    "    \n",
    "general_train = general_train.drop(columns = general_train.columns[:16])\n",
    "general_test = general_test.drop(columns = general_test.columns[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Im-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "      <th>Im-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.053473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>-0.020396</td>\n",
       "      <td>0.191040</td>\n",
       "      <td>-0.006027</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214359</td>\n",
       "      <td>-0.351203</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.059834</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.214359</td>\n",
       "      <td>0.351203</td>\n",
       "      <td>0.247799</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087975</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>-0.104436</td>\n",
       "      <td>-0.161609</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>-0.046500</td>\n",
       "      <td>0.087975</td>\n",
       "      <td>-0.026243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043774</td>\n",
       "      <td>0.338838</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>0.109329</td>\n",
       "      <td>-0.043774</td>\n",
       "      <td>-0.338838</td>\n",
       "      <td>0.191709</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>-0.050767</td>\n",
       "      <td>-0.001937</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180371</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>-0.053805</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.180371</td>\n",
       "      <td>-0.460167</td>\n",
       "      <td>0.508831</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.044245</td>\n",
       "      <td>-0.230247</td>\n",
       "      <td>-0.203451</td>\n",
       "      <td>-0.227503</td>\n",
       "      <td>-0.147676</td>\n",
       "      <td>0.193734</td>\n",
       "      <td>-0.044245</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047762</td>\n",
       "      <td>-0.248549</td>\n",
       "      <td>-0.147676</td>\n",
       "      <td>-0.193734</td>\n",
       "      <td>-0.129608</td>\n",
       "      <td>0.144930</td>\n",
       "      <td>-0.047762</td>\n",
       "      <td>0.248549</td>\n",
       "      <td>0.202009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.057648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070885</td>\n",
       "      <td>-0.019272</td>\n",
       "      <td>-0.058774</td>\n",
       "      <td>-0.123264</td>\n",
       "      <td>-0.113477</td>\n",
       "      <td>-0.131920</td>\n",
       "      <td>0.070885</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397770</td>\n",
       "      <td>-0.108143</td>\n",
       "      <td>-0.113477</td>\n",
       "      <td>0.131920</td>\n",
       "      <td>-0.095433</td>\n",
       "      <td>0.200147</td>\n",
       "      <td>0.397770</td>\n",
       "      <td>0.108143</td>\n",
       "      <td>0.525259</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Im-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  \\\n",
       "0     0.053473          0.0     0.020371    -0.020396     0.191040   \n",
       "1     0.060808          0.0     0.087975     0.026243    -0.104436   \n",
       "2     0.005376          0.0    -0.002020     0.005153    -0.050767   \n",
       "3     0.293753          0.0    -0.044245    -0.230247    -0.203451   \n",
       "4     0.057648          0.0     0.070885    -0.019272    -0.058774   \n",
       "\n",
       "   Im-rho(0,2)  Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  ...  \\\n",
       "0    -0.006027     0.056843    -0.100098     0.020371     0.020396  ...   \n",
       "1    -0.161609     0.097443    -0.046500     0.087975    -0.026243  ...   \n",
       "2    -0.001937     0.020929    -0.047932    -0.002020    -0.005153  ...   \n",
       "3    -0.227503    -0.147676     0.193734    -0.044245     0.230247  ...   \n",
       "4    -0.123264    -0.113477    -0.131920     0.070885     0.019272  ...   \n",
       "\n",
       "   Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  Im-rho(3,0)  Re-rho(3,1)  \\\n",
       "0     0.214359    -0.351203     0.056843     0.100098     0.059834   \n",
       "1    -0.043774     0.338838     0.097443     0.046500     0.120910   \n",
       "2    -0.180371     0.460167     0.020929     0.047932    -0.053805   \n",
       "3    -0.047762    -0.248549    -0.147676    -0.193734    -0.129608   \n",
       "4     0.397770    -0.108143    -0.113477     0.131920    -0.095433   \n",
       "\n",
       "   Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  Re-rho(3,3)  Im-rho(3,3)  \n",
       "0     0.016450     0.214359     0.351203     0.247799          0.0  \n",
       "1     0.109329    -0.043774    -0.338838     0.191709          0.0  \n",
       "2     0.002053    -0.180371    -0.460167     0.508831          0.0  \n",
       "3     0.144930    -0.047762     0.248549     0.202009          0.0  \n",
       "4     0.200147     0.397770     0.108143     0.525259          0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the feature space of the training set\n",
    "general_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Im-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "      <th>Im-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>0.023084</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>-0.070056</td>\n",
       "      <td>-0.102659</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>-0.023084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320404</td>\n",
       "      <td>0.238229</td>\n",
       "      <td>-0.070056</td>\n",
       "      <td>0.102659</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.073372</td>\n",
       "      <td>-0.320404</td>\n",
       "      <td>-0.238229</td>\n",
       "      <td>0.235899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.350981</td>\n",
       "      <td>-0.066885</td>\n",
       "      <td>-0.161386</td>\n",
       "      <td>-0.063365</td>\n",
       "      <td>0.182867</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>-0.350981</td>\n",
       "      <td>0.066885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128470</td>\n",
       "      <td>-0.024482</td>\n",
       "      <td>0.182867</td>\n",
       "      <td>-0.115273</td>\n",
       "      <td>-0.250869</td>\n",
       "      <td>0.098499</td>\n",
       "      <td>-0.128470</td>\n",
       "      <td>0.024482</td>\n",
       "      <td>0.163057</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.157854</td>\n",
       "      <td>0.362676</td>\n",
       "      <td>-0.180470</td>\n",
       "      <td>0.125876</td>\n",
       "      <td>-0.039710</td>\n",
       "      <td>-0.197397</td>\n",
       "      <td>-0.157854</td>\n",
       "      <td>-0.362676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040906</td>\n",
       "      <td>0.093983</td>\n",
       "      <td>-0.039710</td>\n",
       "      <td>0.197397</td>\n",
       "      <td>-0.151127</td>\n",
       "      <td>-0.105409</td>\n",
       "      <td>-0.040906</td>\n",
       "      <td>-0.093983</td>\n",
       "      <td>0.093797</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.092461</td>\n",
       "      <td>-0.300512</td>\n",
       "      <td>-0.212284</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.131786</td>\n",
       "      <td>0.201066</td>\n",
       "      <td>-0.092461</td>\n",
       "      <td>0.300512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054056</td>\n",
       "      <td>-0.175690</td>\n",
       "      <td>0.131786</td>\n",
       "      <td>-0.201066</td>\n",
       "      <td>-0.251215</td>\n",
       "      <td>-0.072701</td>\n",
       "      <td>-0.054056</td>\n",
       "      <td>0.175690</td>\n",
       "      <td>0.199965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.354384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.263113</td>\n",
       "      <td>-0.092900</td>\n",
       "      <td>0.113036</td>\n",
       "      <td>0.283542</td>\n",
       "      <td>-0.009595</td>\n",
       "      <td>-0.240149</td>\n",
       "      <td>-0.263113</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195204</td>\n",
       "      <td>-0.068922</td>\n",
       "      <td>-0.009595</td>\n",
       "      <td>0.240149</td>\n",
       "      <td>0.070078</td>\n",
       "      <td>-0.175784</td>\n",
       "      <td>-0.195204</td>\n",
       "      <td>0.068922</td>\n",
       "      <td>0.162997</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Im-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  \\\n",
       "0     0.065480          0.0    -0.031047     0.023084    -0.008522   \n",
       "1     0.286575          0.0    -0.350981    -0.066885    -0.161386   \n",
       "2     0.432236          0.0    -0.157854     0.362676    -0.180470   \n",
       "3     0.289027          0.0    -0.092461    -0.300512    -0.212284   \n",
       "4     0.354384          0.0    -0.263113    -0.092900     0.113036   \n",
       "\n",
       "   Im-rho(0,2)  Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  ...  \\\n",
       "0     0.210182    -0.070056    -0.102659    -0.031047    -0.023084  ...   \n",
       "1    -0.063365     0.182867     0.115273    -0.350981     0.066885  ...   \n",
       "2     0.125876    -0.039710    -0.197397    -0.157854    -0.362676  ...   \n",
       "3     0.061434     0.131786     0.201066    -0.092461     0.300512  ...   \n",
       "4     0.283542    -0.009595    -0.240149    -0.263113     0.092900  ...   \n",
       "\n",
       "   Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  Im-rho(3,0)  Re-rho(3,1)  \\\n",
       "0    -0.320404     0.238229    -0.070056     0.102659    -0.002975   \n",
       "1    -0.128470    -0.024482     0.182867    -0.115273    -0.250869   \n",
       "2    -0.040906     0.093983    -0.039710     0.197397    -0.151127   \n",
       "3    -0.054056    -0.175690     0.131786    -0.201066    -0.251215   \n",
       "4    -0.195204    -0.068922    -0.009595     0.240149     0.070078   \n",
       "\n",
       "   Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  Re-rho(3,3)  Im-rho(3,3)  \n",
       "0    -0.073372    -0.320404    -0.238229     0.235899          0.0  \n",
       "1     0.098499    -0.128470     0.024482     0.163057          0.0  \n",
       "2    -0.105409    -0.040906    -0.093983     0.093797          0.0  \n",
       "3    -0.072701    -0.054056     0.175690     0.199965          0.0  \n",
       "4    -0.175784    -0.195204     0.068922     0.162997          0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the feature space of the test set\n",
    "general_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with zero variance\n",
    "\n",
    "zero_var_cols = [] # empty array \n",
    "\n",
    "for item in general_train.columns:\n",
    "    if general_train[item].var() == 0:\n",
    "        zero_var_cols.append(item) # append zero-variance columns to the empty array \n",
    "    \n",
    "general_train = general_train.drop(columns = zero_var_cols)    \n",
    "general_test = general_test.drop(columns = zero_var_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.053473</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>-0.020396</td>\n",
       "      <td>0.191040</td>\n",
       "      <td>-0.006027</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683188</td>\n",
       "      <td>0.214359</td>\n",
       "      <td>-0.351203</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.059834</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.214359</td>\n",
       "      <td>0.351203</td>\n",
       "      <td>0.247799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060808</td>\n",
       "      <td>0.087975</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>-0.104436</td>\n",
       "      <td>-0.161609</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>-0.046500</td>\n",
       "      <td>0.087975</td>\n",
       "      <td>-0.026243</td>\n",
       "      <td>0.138606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608877</td>\n",
       "      <td>-0.043774</td>\n",
       "      <td>0.338838</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>0.109329</td>\n",
       "      <td>-0.043774</td>\n",
       "      <td>-0.338838</td>\n",
       "      <td>0.191709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>-0.050767</td>\n",
       "      <td>-0.001937</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480095</td>\n",
       "      <td>-0.180371</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>-0.053805</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.180371</td>\n",
       "      <td>-0.460167</td>\n",
       "      <td>0.508831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293753</td>\n",
       "      <td>-0.044245</td>\n",
       "      <td>-0.230247</td>\n",
       "      <td>-0.203451</td>\n",
       "      <td>-0.227503</td>\n",
       "      <td>-0.147676</td>\n",
       "      <td>0.193734</td>\n",
       "      <td>-0.044245</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>0.187134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317103</td>\n",
       "      <td>-0.047762</td>\n",
       "      <td>-0.248549</td>\n",
       "      <td>-0.147676</td>\n",
       "      <td>-0.193734</td>\n",
       "      <td>-0.129608</td>\n",
       "      <td>0.144930</td>\n",
       "      <td>-0.047762</td>\n",
       "      <td>0.248549</td>\n",
       "      <td>0.202009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.057648</td>\n",
       "      <td>0.070885</td>\n",
       "      <td>-0.019272</td>\n",
       "      <td>-0.058774</td>\n",
       "      <td>-0.123264</td>\n",
       "      <td>-0.113477</td>\n",
       "      <td>-0.131920</td>\n",
       "      <td>0.070885</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.093604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323489</td>\n",
       "      <td>0.397770</td>\n",
       "      <td>-0.108143</td>\n",
       "      <td>-0.113477</td>\n",
       "      <td>0.131920</td>\n",
       "      <td>-0.095433</td>\n",
       "      <td>0.200147</td>\n",
       "      <td>0.397770</td>\n",
       "      <td>0.108143</td>\n",
       "      <td>0.525259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  Im-rho(0,2)  \\\n",
       "0     0.053473     0.020371    -0.020396     0.191040    -0.006027   \n",
       "1     0.060808     0.087975     0.026243    -0.104436    -0.161609   \n",
       "2     0.005376    -0.002020     0.005153    -0.050767    -0.001937   \n",
       "3     0.293753    -0.044245    -0.230247    -0.203451    -0.227503   \n",
       "4     0.057648     0.070885    -0.019272    -0.058774    -0.123264   \n",
       "\n",
       "   Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  Re-rho(1,1)  ...  \\\n",
       "0     0.056843    -0.100098     0.020371     0.020396     0.015540  ...   \n",
       "1     0.097443    -0.046500     0.087975    -0.026243     0.138606  ...   \n",
       "2     0.020929    -0.047932    -0.002020    -0.005153     0.005698  ...   \n",
       "3    -0.147676     0.193734    -0.044245     0.230247     0.187134  ...   \n",
       "4    -0.113477    -0.131920     0.070885     0.019272     0.093604  ...   \n",
       "\n",
       "   Re-rho(2,2)  Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  Im-rho(3,0)  \\\n",
       "0     0.683188     0.214359    -0.351203     0.056843     0.100098   \n",
       "1     0.608877    -0.043774     0.338838     0.097443     0.046500   \n",
       "2     0.480095    -0.180371     0.460167     0.020929     0.047932   \n",
       "3     0.317103    -0.047762    -0.248549    -0.147676    -0.193734   \n",
       "4     0.323489     0.397770    -0.108143    -0.113477     0.131920   \n",
       "\n",
       "   Re-rho(3,1)  Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  Re-rho(3,3)  \n",
       "0     0.059834     0.016450     0.214359     0.351203     0.247799  \n",
       "1     0.120910     0.109329    -0.043774    -0.338838     0.191709  \n",
       "2    -0.053805     0.002053    -0.180371    -0.460167     0.508831  \n",
       "3    -0.129608     0.144930    -0.047762     0.248549     0.202009  \n",
       "4    -0.095433     0.200147     0.397770     0.108143     0.525259  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display training dataset without zero-variance columns\n",
    "general_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065480</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>0.023084</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>-0.070056</td>\n",
       "      <td>-0.102659</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>-0.023084</td>\n",
       "      <td>0.022858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675762</td>\n",
       "      <td>-0.320404</td>\n",
       "      <td>0.238229</td>\n",
       "      <td>-0.070056</td>\n",
       "      <td>0.102659</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.073372</td>\n",
       "      <td>-0.320404</td>\n",
       "      <td>-0.238229</td>\n",
       "      <td>0.235899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286575</td>\n",
       "      <td>-0.350981</td>\n",
       "      <td>-0.066885</td>\n",
       "      <td>-0.161386</td>\n",
       "      <td>-0.063365</td>\n",
       "      <td>0.182867</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>-0.350981</td>\n",
       "      <td>0.066885</td>\n",
       "      <td>0.445472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104896</td>\n",
       "      <td>-0.128470</td>\n",
       "      <td>-0.024482</td>\n",
       "      <td>0.182867</td>\n",
       "      <td>-0.115273</td>\n",
       "      <td>-0.250869</td>\n",
       "      <td>0.098499</td>\n",
       "      <td>-0.128470</td>\n",
       "      <td>0.024482</td>\n",
       "      <td>0.163057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432236</td>\n",
       "      <td>-0.157854</td>\n",
       "      <td>0.362676</td>\n",
       "      <td>-0.180470</td>\n",
       "      <td>0.125876</td>\n",
       "      <td>-0.039710</td>\n",
       "      <td>-0.197397</td>\n",
       "      <td>-0.157854</td>\n",
       "      <td>-0.362676</td>\n",
       "      <td>0.361958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112009</td>\n",
       "      <td>-0.040906</td>\n",
       "      <td>0.093983</td>\n",
       "      <td>-0.039710</td>\n",
       "      <td>0.197397</td>\n",
       "      <td>-0.151127</td>\n",
       "      <td>-0.105409</td>\n",
       "      <td>-0.040906</td>\n",
       "      <td>-0.093983</td>\n",
       "      <td>0.093797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289027</td>\n",
       "      <td>-0.092461</td>\n",
       "      <td>-0.300512</td>\n",
       "      <td>-0.212284</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.131786</td>\n",
       "      <td>0.201066</td>\n",
       "      <td>-0.092461</td>\n",
       "      <td>0.300512</td>\n",
       "      <td>0.342033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168976</td>\n",
       "      <td>-0.054056</td>\n",
       "      <td>-0.175690</td>\n",
       "      <td>0.131786</td>\n",
       "      <td>-0.201066</td>\n",
       "      <td>-0.251215</td>\n",
       "      <td>-0.072701</td>\n",
       "      <td>-0.054056</td>\n",
       "      <td>0.175690</td>\n",
       "      <td>0.199965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.354384</td>\n",
       "      <td>-0.263113</td>\n",
       "      <td>-0.092900</td>\n",
       "      <td>0.113036</td>\n",
       "      <td>0.283542</td>\n",
       "      <td>-0.009595</td>\n",
       "      <td>-0.240149</td>\n",
       "      <td>-0.263113</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.219702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262917</td>\n",
       "      <td>-0.195204</td>\n",
       "      <td>-0.068922</td>\n",
       "      <td>-0.009595</td>\n",
       "      <td>0.240149</td>\n",
       "      <td>0.070078</td>\n",
       "      <td>-0.175784</td>\n",
       "      <td>-0.195204</td>\n",
       "      <td>0.068922</td>\n",
       "      <td>0.162997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Re-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  Im-rho(0,2)  \\\n",
       "0     0.065480    -0.031047     0.023084    -0.008522     0.210182   \n",
       "1     0.286575    -0.350981    -0.066885    -0.161386    -0.063365   \n",
       "2     0.432236    -0.157854     0.362676    -0.180470     0.125876   \n",
       "3     0.289027    -0.092461    -0.300512    -0.212284     0.061434   \n",
       "4     0.354384    -0.263113    -0.092900     0.113036     0.283542   \n",
       "\n",
       "   Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  Re-rho(1,1)  ...  \\\n",
       "0    -0.070056    -0.102659    -0.031047    -0.023084     0.022858  ...   \n",
       "1     0.182867     0.115273    -0.350981     0.066885     0.445472  ...   \n",
       "2    -0.039710    -0.197397    -0.157854    -0.362676     0.361958  ...   \n",
       "3     0.131786     0.201066    -0.092461     0.300512     0.342033  ...   \n",
       "4    -0.009595    -0.240149    -0.263113     0.092900     0.219702  ...   \n",
       "\n",
       "   Re-rho(2,2)  Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  Im-rho(3,0)  \\\n",
       "0     0.675762    -0.320404     0.238229    -0.070056     0.102659   \n",
       "1     0.104896    -0.128470    -0.024482     0.182867    -0.115273   \n",
       "2     0.112009    -0.040906     0.093983    -0.039710     0.197397   \n",
       "3     0.168976    -0.054056    -0.175690     0.131786    -0.201066   \n",
       "4     0.262917    -0.195204    -0.068922    -0.009595     0.240149   \n",
       "\n",
       "   Re-rho(3,1)  Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  Re-rho(3,3)  \n",
       "0    -0.002975    -0.073372    -0.320404    -0.238229     0.235899  \n",
       "1    -0.250869     0.098499    -0.128470     0.024482     0.163057  \n",
       "2    -0.151127    -0.105409    -0.040906    -0.093983     0.093797  \n",
       "3    -0.251215    -0.072701    -0.054056     0.175690     0.199965  \n",
       "4     0.070078    -0.175784    -0.195204     0.068922     0.162997  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display test dataset without zero-variance columns\n",
    "general_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset, equal real parts for the symmetric components rho(0,1)\n",
      "Training dataset, opposite imaginary parts for the symmetric components rho(0,1)\n",
      "Training dataset, equal real parts for the symmetric components rho(0,2)\n",
      "Training dataset, opposite imaginary parts for the symmetric components rho(0,2)\n",
      "Training dataset, equal real parts for the symmetric components rho(0,3)\n",
      "Training dataset, opposite imaginary parts for the symmetric components rho(0,3)\n",
      "Training dataset, equal real parts for the symmetric components rho(1,2)\n",
      "Training dataset, opposite imaginary parts for the symmetric components rho(1,2)\n",
      "Training dataset, equal real parts for the symmetric components rho(1,3)\n",
      "Training dataset, opposite imaginary parts for the symmetric components rho(1,3)\n",
      "Training dataset, equal real parts for the symmetric components rho(2,3)\n",
      "Training dataset, opposite imaginary parts for the symmetric components rho(2,3)\n",
      "Test dataset, equal real parts for the symmetric components rho(0,1)\n",
      "Test dataset, opposite imaginary parts for the symmetric components rho(0,1)\n",
      "Test dataset, equal real parts for the symmetric components rho(0,2)\n",
      "Test dataset, opposite imaginary parts for the symmetric components rho(0,2)\n",
      "Test dataset, equal real parts for the symmetric components rho(0,3)\n",
      "Test dataset, opposite imaginary parts for the symmetric components rho(0,3)\n",
      "Test dataset, equal real parts for the symmetric components rho(1,2)\n",
      "Test dataset, opposite imaginary parts for the symmetric components rho(1,2)\n",
      "Test dataset, equal real parts for the symmetric components rho(1,3)\n",
      "Test dataset, opposite imaginary parts for the symmetric components rho(1,3)\n",
      "Test dataset, equal real parts for the symmetric components rho(2,3)\n",
      "Test dataset, opposite imaginary parts for the symmetric components rho(2,3)\n"
     ]
    }
   ],
   "source": [
    "# Let us check if we have equal real parts and opposite imaginary parts in the feature space\n",
    "\n",
    "# Training set \n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "        if all(general_train[[\"Re-rho(\" + str(i) + \",\" + str(j) +\")\"]].values == general_train[[\"Re-rho(\" + str(j) + \",\" + str(i) +\")\"]].values):\n",
    "            print(\"Training dataset, equal real parts for the symmetric components rho(\"+str(i)+\",\"+str(j)+\")\")\n",
    "        if all(general_train[[\"Im-rho(\" + str(i) + \",\" + str(j) +\")\"]].values == - general_train[[\"Im-rho(\" + str(j) + \",\" + str(i) +\")\"]].values):\n",
    "            print(\"Training dataset, opposite imaginary parts for the symmetric components rho(\"+str(i)+\",\"+str(j)+\")\")\n",
    "\n",
    "            \n",
    "# Test set\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "        if all(general_test[[\"Re-rho(\" + str(i) + \",\" + str(j) +\")\"]].values == general_test[[\"Re-rho(\" + str(j) + \",\" + str(i) +\")\"]].values):\n",
    "            print(\"Test dataset, equal real parts for the symmetric components rho(\"+str(i)+\",\"+str(j)+\")\")\n",
    "        if all(general_test[[\"Im-rho(\" + str(i) + \",\" + str(j) +\")\"]].values == - general_test[[\"Im-rho(\" + str(j) + \",\" + str(i) +\")\"]].values):\n",
    "            print(\"Test dataset, opposite imaginary parts for the symmetric components rho(\"+str(i)+\",\"+str(j)+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Re-rho(0,0)</th>\n",
       "      <th>Re-rho(0,1)</th>\n",
       "      <th>Im-rho(0,1)</th>\n",
       "      <th>Re-rho(0,2)</th>\n",
       "      <th>Im-rho(0,2)</th>\n",
       "      <th>Re-rho(0,3)</th>\n",
       "      <th>Im-rho(0,3)</th>\n",
       "      <th>Re-rho(1,0)</th>\n",
       "      <th>Im-rho(1,0)</th>\n",
       "      <th>Re-rho(1,1)</th>\n",
       "      <th>...</th>\n",
       "      <th>Re-rho(2,2)</th>\n",
       "      <th>Re-rho(2,3)</th>\n",
       "      <th>Im-rho(2,3)</th>\n",
       "      <th>Re-rho(3,0)</th>\n",
       "      <th>Im-rho(3,0)</th>\n",
       "      <th>Re-rho(3,1)</th>\n",
       "      <th>Im-rho(3,1)</th>\n",
       "      <th>Re-rho(3,2)</th>\n",
       "      <th>Im-rho(3,2)</th>\n",
       "      <th>Re-rho(3,3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Re-rho(0,0)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>-0.006788</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>-0.000680</td>\n",
       "      <td>-0.222339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213425</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-0.006788</td>\n",
       "      <td>-0.004553</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>-0.002479</td>\n",
       "      <td>-0.572114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(0,1)</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000350</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>-0.003240</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.003952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(0,1)</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>-0.002050</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.003684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>-0.002188</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>-0.009132</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>-0.002188</td>\n",
       "      <td>-0.377292</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(0,2)</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>-0.017383</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.003922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.369995</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>-0.012966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(0,2)</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>-0.002050</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004621</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005926</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.007008</td>\n",
       "      <td>-0.004621</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.374140</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>-0.002897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(0,3)</td>\n",
       "      <td>-0.006788</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>-0.004621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009532</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>-0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(0,3)</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>-0.017383</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>-0.009532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>-0.009132</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003656</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>-0.009532</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.005135</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>-0.000831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,0)</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000350</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>-0.003240</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.003952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(1,0)</td>\n",
       "      <td>-0.000680</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>-0.009132</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002978</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>-0.377292</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>-0.007331</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,1)</td>\n",
       "      <td>-0.222339</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>-0.003684</td>\n",
       "      <td>-0.003922</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571320</td>\n",
       "      <td>-0.003957</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>-0.003957</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>-0.200227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,2)</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>-0.011422</td>\n",
       "      <td>-0.007948</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.011942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.001417</td>\n",
       "      <td>-0.011422</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>-0.010748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(1,2)</td>\n",
       "      <td>-0.002462</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>-0.004082</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>-0.006372</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>-0.002370</td>\n",
       "      <td>-0.010664</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>-0.002439</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>-0.002370</td>\n",
       "      <td>0.010664</td>\n",
       "      <td>-0.005255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(1,3)</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.369995</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>-0.005135</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(1,3)</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>-0.007331</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>0.374140</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>-0.000381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.007693</td>\n",
       "      <td>-0.009838</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-0.004848</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.007693</td>\n",
       "      <td>0.009838</td>\n",
       "      <td>-0.001206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,0)</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>-0.017383</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.003922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.369995</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>-0.012966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(2,0)</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.004621</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>-0.002050</td>\n",
       "      <td>-0.003515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.004621</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.374140</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.007008</td>\n",
       "      <td>0.002897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,1)</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>-0.011422</td>\n",
       "      <td>-0.007948</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.011942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.001417</td>\n",
       "      <td>-0.011422</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>-0.010748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(2,1)</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>-0.004082</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.010664</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>-0.001590</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>-0.010664</td>\n",
       "      <td>0.005255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,2)</td>\n",
       "      <td>-0.213425</td>\n",
       "      <td>-0.000350</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>-0.005926</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.003656</td>\n",
       "      <td>-0.000350</td>\n",
       "      <td>-0.002978</td>\n",
       "      <td>-0.571320</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>-0.220518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(2,3)</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>-0.002188</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>-0.003957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>-0.002496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(2,3)</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-0.003240</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>-0.007008</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>-0.003240</td>\n",
       "      <td>-0.377292</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>0.009838</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.009541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(3,0)</td>\n",
       "      <td>-0.006788</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>-0.004621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009532</td>\n",
       "      <td>-0.009462</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>-0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(3,0)</td>\n",
       "      <td>-0.004553</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>-0.009132</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.000831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(3,1)</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.369995</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>-0.005135</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(3,1)</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>-0.374140</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>-0.007331</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.009838</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>-0.009838</td>\n",
       "      <td>0.001206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(3,2)</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>-0.002188</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>-0.003957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>-0.002496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Im-rho(3,2)</td>\n",
       "      <td>-0.002479</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>-0.377292</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.009838</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Re-rho(3,3)</td>\n",
       "      <td>-0.572114</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.012966</td>\n",
       "      <td>-0.002897</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>-0.000831</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.200227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220518</td>\n",
       "      <td>-0.002496</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>-0.002496</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Re-rho(0,0)  Re-rho(0,1)  Im-rho(0,1)  Re-rho(0,2)  Im-rho(0,2)  \\\n",
       "Re-rho(0,0)     1.000000     0.000250     0.000680     0.006801     0.005300   \n",
       "Re-rho(0,1)     0.000250     1.000000    -0.000686    -0.001112    -0.002623   \n",
       "Im-rho(0,1)     0.000680    -0.000686     1.000000     0.001095    -0.002050   \n",
       "Re-rho(0,2)     0.006801    -0.001112     0.001095     1.000000    -0.002719   \n",
       "Im-rho(0,2)     0.005300    -0.002623    -0.002050    -0.002719     1.000000   \n",
       "Re-rho(0,3)    -0.006788    -0.009462    -0.002962     0.007383    -0.004621   \n",
       "Im-rho(0,3)     0.004553    -0.003157     0.009132    -0.017383     0.006659   \n",
       "Re-rho(1,0)     0.000250     1.000000    -0.000686    -0.001112    -0.002623   \n",
       "Im-rho(1,0)    -0.000680     0.000686    -1.000000    -0.001095     0.002050   \n",
       "Re-rho(1,1)    -0.222339    -0.003851    -0.003684    -0.003922     0.003515   \n",
       "Re-rho(1,2)    -0.002074     0.005672    -0.000844    -0.002297    -0.011818   \n",
       "Im-rho(1,2)    -0.002462    -0.001990    -0.004082    -0.001675    -0.006372   \n",
       "Re-rho(1,3)     0.002957     0.001712     0.004606     0.369995    -0.002975   \n",
       "Im-rho(1,3)     0.002173     0.002979    -0.007331    -0.003883     0.374140   \n",
       "Re-rho(2,0)     0.006801    -0.001112     0.001095     1.000000    -0.002719   \n",
       "Im-rho(2,0)    -0.005300     0.002623     0.002050     0.002719    -1.000000   \n",
       "Re-rho(2,1)    -0.002074     0.005672    -0.000844    -0.002297    -0.011818   \n",
       "Im-rho(2,1)     0.002462     0.001990     0.004082     0.001675     0.006372   \n",
       "Re-rho(2,2)    -0.213425    -0.000350     0.002978     0.009949    -0.005926   \n",
       "Re-rho(2,3)     0.006366     0.373721    -0.002188    -0.005747    -0.000031   \n",
       "Im-rho(2,3)     0.002479    -0.003240     0.377292     0.004205    -0.007008   \n",
       "Re-rho(3,0)    -0.006788    -0.009462    -0.002962     0.007383    -0.004621   \n",
       "Im-rho(3,0)    -0.004553     0.003157    -0.009132     0.017383    -0.006659   \n",
       "Re-rho(3,1)     0.002957     0.001712     0.004606     0.369995    -0.002975   \n",
       "Im-rho(3,1)    -0.002173    -0.002979     0.007331     0.003883    -0.374140   \n",
       "Re-rho(3,2)     0.006366     0.373721    -0.002188    -0.005747    -0.000031   \n",
       "Im-rho(3,2)    -0.002479     0.003240    -0.377292    -0.004205     0.007008   \n",
       "Re-rho(3,3)    -0.572114     0.003952    -0.000002    -0.012966    -0.002897   \n",
       "\n",
       "             Re-rho(0,3)  Im-rho(0,3)  Re-rho(1,0)  Im-rho(1,0)  Re-rho(1,1)  \\\n",
       "Re-rho(0,0)    -0.006788     0.004553     0.000250    -0.000680    -0.222339   \n",
       "Re-rho(0,1)    -0.009462    -0.003157     1.000000     0.000686    -0.003851   \n",
       "Im-rho(0,1)    -0.002962     0.009132    -0.000686    -1.000000    -0.003684   \n",
       "Re-rho(0,2)     0.007383    -0.017383    -0.001112    -0.001095    -0.003922   \n",
       "Im-rho(0,2)    -0.004621     0.006659    -0.002623     0.002050     0.003515   \n",
       "Re-rho(0,3)     1.000000    -0.009532    -0.009462     0.002962     0.007742   \n",
       "Im-rho(0,3)    -0.009532     1.000000    -0.003157    -0.009132    -0.000082   \n",
       "Re-rho(1,0)    -0.009462    -0.003157     1.000000     0.000686    -0.003851   \n",
       "Im-rho(1,0)     0.002962    -0.009132     0.000686     1.000000     0.003684   \n",
       "Re-rho(1,1)     0.007742    -0.000082    -0.003851     0.003684     1.000000   \n",
       "Re-rho(1,2)    -0.011422    -0.007948     0.005672     0.000844     0.011942   \n",
       "Im-rho(1,2)    -0.000978     0.001571    -0.001990     0.004082     0.000601   \n",
       "Re-rho(1,3)    -0.008988    -0.005135     0.001712    -0.004606    -0.013122   \n",
       "Im-rho(1,3)     0.008802     0.005794     0.002979     0.007331    -0.000381   \n",
       "Re-rho(2,0)     0.007383    -0.017383    -0.001112    -0.001095    -0.003922   \n",
       "Im-rho(2,0)     0.004621    -0.006659     0.002623    -0.002050    -0.003515   \n",
       "Re-rho(2,1)    -0.011422    -0.007948     0.005672     0.000844     0.011942   \n",
       "Im-rho(2,1)     0.000978    -0.001571     0.001990    -0.004082    -0.000601   \n",
       "Re-rho(2,2)    -0.000203    -0.003656    -0.000350    -0.002978    -0.571320   \n",
       "Re-rho(2,3)    -0.003186    -0.007614     0.373721     0.002188    -0.003957   \n",
       "Im-rho(2,3)     0.003504     0.001259    -0.003240    -0.377292     0.003588   \n",
       "Re-rho(3,0)     1.000000    -0.009532    -0.009462     0.002962     0.007742   \n",
       "Im-rho(3,0)     0.009532    -1.000000     0.003157     0.009132     0.000082   \n",
       "Re-rho(3,1)    -0.008988    -0.005135     0.001712    -0.004606    -0.013122   \n",
       "Im-rho(3,1)    -0.008802    -0.005794    -0.002979    -0.007331     0.000381   \n",
       "Re-rho(3,2)    -0.003186    -0.007614     0.373721     0.002188    -0.003957   \n",
       "Im-rho(3,2)    -0.003504    -0.001259     0.003240     0.377292    -0.003588   \n",
       "Re-rho(3,3)    -0.000687    -0.000831     0.003952     0.000002    -0.200227   \n",
       "\n",
       "             ...  Re-rho(2,2)  Re-rho(2,3)  Im-rho(2,3)  Re-rho(3,0)  \\\n",
       "Re-rho(0,0)  ...    -0.213425     0.006366     0.002479    -0.006788   \n",
       "Re-rho(0,1)  ...    -0.000350     0.373721    -0.003240    -0.009462   \n",
       "Im-rho(0,1)  ...     0.002978    -0.002188     0.377292    -0.002962   \n",
       "Re-rho(0,2)  ...     0.009949    -0.005747     0.004205     0.007383   \n",
       "Im-rho(0,2)  ...    -0.005926    -0.000031    -0.007008    -0.004621   \n",
       "Re-rho(0,3)  ...    -0.000203    -0.003186     0.003504     1.000000   \n",
       "Im-rho(0,3)  ...    -0.003656    -0.007614     0.001259    -0.009532   \n",
       "Re-rho(1,0)  ...    -0.000350     0.373721    -0.003240    -0.009462   \n",
       "Im-rho(1,0)  ...    -0.002978     0.002188    -0.377292     0.002962   \n",
       "Re-rho(1,1)  ...    -0.571320    -0.003957     0.003588     0.007742   \n",
       "Re-rho(1,2)  ...     0.000889    -0.002908    -0.001417    -0.011422   \n",
       "Im-rho(1,2)  ...     0.007086    -0.002370    -0.010664    -0.000978   \n",
       "Re-rho(1,3)  ...     0.009175    -0.001932    -0.001349    -0.008988   \n",
       "Im-rho(1,3)  ...    -0.000601    -0.007693    -0.009838     0.008802   \n",
       "Re-rho(2,0)  ...     0.009949    -0.005747     0.004205     0.007383   \n",
       "Im-rho(2,0)  ...     0.005926     0.000031     0.007008     0.004621   \n",
       "Re-rho(2,1)  ...     0.000889    -0.002908    -0.001417    -0.011422   \n",
       "Im-rho(2,1)  ...    -0.007086     0.002370     0.010664     0.000978   \n",
       "Re-rho(2,2)  ...     1.000000     0.000027     0.003424    -0.000203   \n",
       "Re-rho(2,3)  ...     0.000027     1.000000    -0.009704    -0.003186   \n",
       "Im-rho(2,3)  ...     0.003424    -0.009704     1.000000     0.003504   \n",
       "Re-rho(3,0)  ...    -0.000203    -0.003186     0.003504     1.000000   \n",
       "Im-rho(3,0)  ...     0.003656     0.007614    -0.001259     0.009532   \n",
       "Re-rho(3,1)  ...     0.009175    -0.001932    -0.001349    -0.008988   \n",
       "Im-rho(3,1)  ...     0.000601     0.007693     0.009838    -0.008802   \n",
       "Re-rho(3,2)  ...     0.000027     1.000000    -0.009704    -0.003186   \n",
       "Im-rho(3,2)  ...    -0.003424     0.009704    -1.000000    -0.003504   \n",
       "Re-rho(3,3)  ...    -0.220518    -0.002496    -0.009541    -0.000687   \n",
       "\n",
       "             Im-rho(3,0)  Re-rho(3,1)  Im-rho(3,1)  Re-rho(3,2)  Im-rho(3,2)  \\\n",
       "Re-rho(0,0)    -0.004553     0.002957    -0.002173     0.006366    -0.002479   \n",
       "Re-rho(0,1)     0.003157     0.001712    -0.002979     0.373721     0.003240   \n",
       "Im-rho(0,1)    -0.009132     0.004606     0.007331    -0.002188    -0.377292   \n",
       "Re-rho(0,2)     0.017383     0.369995     0.003883    -0.005747    -0.004205   \n",
       "Im-rho(0,2)    -0.006659    -0.002975    -0.374140    -0.000031     0.007008   \n",
       "Re-rho(0,3)     0.009532    -0.008988    -0.008802    -0.003186    -0.003504   \n",
       "Im-rho(0,3)    -1.000000    -0.005135    -0.005794    -0.007614    -0.001259   \n",
       "Re-rho(1,0)     0.003157     0.001712    -0.002979     0.373721     0.003240   \n",
       "Im-rho(1,0)     0.009132    -0.004606    -0.007331     0.002188     0.377292   \n",
       "Re-rho(1,1)     0.000082    -0.013122     0.000381    -0.003957    -0.003588   \n",
       "Re-rho(1,2)     0.007948     0.002524     0.013495    -0.002908     0.001417   \n",
       "Im-rho(1,2)    -0.001571    -0.002439     0.001590    -0.002370     0.010664   \n",
       "Re-rho(1,3)     0.005135     1.000000     0.004848    -0.001932     0.001349   \n",
       "Im-rho(1,3)    -0.005794    -0.004848    -1.000000    -0.007693     0.009838   \n",
       "Re-rho(2,0)     0.017383     0.369995     0.003883    -0.005747    -0.004205   \n",
       "Im-rho(2,0)     0.006659     0.002975     0.374140     0.000031    -0.007008   \n",
       "Re-rho(2,1)     0.007948     0.002524     0.013495    -0.002908     0.001417   \n",
       "Im-rho(2,1)     0.001571     0.002439    -0.001590     0.002370    -0.010664   \n",
       "Re-rho(2,2)     0.003656     0.009175     0.000601     0.000027    -0.003424   \n",
       "Re-rho(2,3)     0.007614    -0.001932     0.007693     1.000000     0.009704   \n",
       "Im-rho(2,3)    -0.001259    -0.001349     0.009838    -0.009704    -1.000000   \n",
       "Re-rho(3,0)     0.009532    -0.008988    -0.008802    -0.003186    -0.003504   \n",
       "Im-rho(3,0)     1.000000     0.005135     0.005794     0.007614     0.001259   \n",
       "Re-rho(3,1)     0.005135     1.000000     0.004848    -0.001932     0.001349   \n",
       "Im-rho(3,1)     0.005794     0.004848     1.000000     0.007693    -0.009838   \n",
       "Re-rho(3,2)     0.007614    -0.001932     0.007693     1.000000     0.009704   \n",
       "Im-rho(3,2)     0.001259     0.001349    -0.009838     0.009704     1.000000   \n",
       "Re-rho(3,3)     0.000831     0.000897     0.001206    -0.002496     0.009541   \n",
       "\n",
       "             Re-rho(3,3)  \n",
       "Re-rho(0,0)    -0.572114  \n",
       "Re-rho(0,1)     0.003952  \n",
       "Im-rho(0,1)    -0.000002  \n",
       "Re-rho(0,2)    -0.012966  \n",
       "Im-rho(0,2)    -0.002897  \n",
       "Re-rho(0,3)    -0.000687  \n",
       "Im-rho(0,3)    -0.000831  \n",
       "Re-rho(1,0)     0.003952  \n",
       "Im-rho(1,0)     0.000002  \n",
       "Re-rho(1,1)    -0.200227  \n",
       "Re-rho(1,2)    -0.010748  \n",
       "Im-rho(1,2)    -0.005255  \n",
       "Re-rho(1,3)     0.000897  \n",
       "Im-rho(1,3)    -0.001206  \n",
       "Re-rho(2,0)    -0.012966  \n",
       "Im-rho(2,0)     0.002897  \n",
       "Re-rho(2,1)    -0.010748  \n",
       "Im-rho(2,1)     0.005255  \n",
       "Re-rho(2,2)    -0.220518  \n",
       "Re-rho(2,3)    -0.002496  \n",
       "Im-rho(2,3)    -0.009541  \n",
       "Re-rho(3,0)    -0.000687  \n",
       "Im-rho(3,0)     0.000831  \n",
       "Re-rho(3,1)     0.000897  \n",
       "Im-rho(3,1)     0.001206  \n",
       "Re-rho(3,2)    -0.002496  \n",
       "Im-rho(3,2)     0.009541  \n",
       "Re-rho(3,3)     1.000000  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the correlation matrix of the training set and of the test set\n",
    "\n",
    "general_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is equal to $\\pm 1$ or is of the order $O(1)$ among diagonal components or among symmetric components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting features and labels from both training and the test set into numpy arrays\n",
    "\n",
    "training_dt = general_train.values\n",
    "training_targets = labels_general_train.values\n",
    "\n",
    "test_dt = general_test.values\n",
    "test_targets = labels_general_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam \n",
    "    \n",
    "def ff_net(features_train, targets_train, features_test, targets_test): # build the model \n",
    "        \n",
    "    np.random.seed(0) # Fix a random seed realization \n",
    "    model = Sequential()  \n",
    "    model.add(Dense(15, activation = \"tanh\")) # 1st hidden layer, made of 15 neurons with the tanh activation function\n",
    "    model.add(Dense(15, activation = \"tanh\")) # 2nd hidden layer, made of 15 neurons with the tanh activation function\n",
    "    model.add(Dense(1, activation = \"sigmoid\")) # Output layer, made of 1 neuron and with the sigmoid activation function\n",
    "\n",
    "    # Tune the model hyperparameters\n",
    "    \n",
    "    # The optimizer is the standard Adam gradient descent with the binary cross entropy loss function, compatible with\n",
    "    # the sigmoid activation function on the output layer \n",
    "    \n",
    "    opt = Adam(lr = 0.00001, beta_1 = 0.99, beta_2 = 0.999)\n",
    "    model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
    "    history = model.fit(features_train, targets_train, batch_size = 50, epochs = 1000, verbose = 0)\n",
    "    loss_train, acc_train = model.evaluate(features_train, targets_train)\n",
    "    loss_test, acc_test = model.evaluate(features_test, targets_test)\n",
    "    \n",
    "    # Show the trend loss function vs. # of epochs and accuracy vs. # of epochs\n",
    "    \n",
    "    plt.plot(history.history[\"binary_accuracy\"])\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.xlabel(\"Number of epochs\", fontsize = 25)\n",
    "    plt.yscale(\"log\") # logarithmic scale on both axes\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.legend([\"accuracy\",\"loss\"], loc = 7, fontsize = 14)\n",
    "\n",
    "    # The model output contains the accuracy on the training and on the test set. A comparison between them\n",
    "    # could help us understand whether there has been any overfitting. \n",
    "    # Along with this, we compute the model weights and the predicted classes\n",
    "    \n",
    "    return acc_train, acc_test, model.predict_classes(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s 18us/step\n",
      "5000/5000 [==============================] - 0s 18us/step\n",
      "Time to run the script: 338.65 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "net_output = ff_net(training_dt, training_targets, test_dt, test_targets)\n",
    "print(\"Time to run the script: {} seconds\".format(round(time.time()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracies on the training set and on the test set respectively are 97.0 %, 97.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracies on the training set and on the test set respectively are {} %, {} %\".format(round(net_output[0],2)*100, round(net_output[1],2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of misclassified targets is 167 out of 5000\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the model predictions\n",
    "\n",
    "pr = net_output[2]\n",
    "\n",
    "predlabels = []\n",
    "\n",
    "for i in range(len(pr)):\n",
    "    predlabels.append(pr[i][0])\n",
    "\n",
    "# Convert the numpy array of the test labels into a pandas dataframe\n",
    "    \n",
    "test_labels = pd.DataFrame(test_targets)    \n",
    "\n",
    "predlabels = pd.DataFrame(predlabels)\n",
    "\n",
    "# Build a dataset composed of: p_sym from the test set, labels from the test set, predicted labels from the model\n",
    "\n",
    "total = pd.concat([test_labels, predlabels], axis=1, ignore_index=True)\n",
    "total.columns = [\"actual_label\", \"pred_label\"]\n",
    "total\n",
    "# Isolate the wrong predictions from the entire dataset\n",
    "\n",
    "wrong = total.loc[total['actual_label'] != total['pred_label']]\n",
    "wrong\n",
    "# Print the number of misclassified data\n",
    "\n",
    "print(\"The number of misclassified targets is {} out of 5000\".format(wrong[\"actual_label\"].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the case of Werner states, quantum entanglement of general states does not depend on one parameter but it is a high-dimensional problem. Therefore, it is not possible to give a 2D or 3D representation of such parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of principal components that enclose most variance is 15\n"
     ]
    }
   ],
   "source": [
    "pca_20 = PCA(n_components = 20)\n",
    "pca_20.fit(training_dt)\n",
    "\n",
    "# How many components should I retain? \n",
    "\n",
    "count = 0\n",
    "\n",
    "for item in pca_20.explained_variance_ratio_:\n",
    "    if item >= 10**(-20):\n",
    "        count += 1\n",
    "        \n",
    "print(\"The number of principal components that enclose most variance is {}\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA reduction we have just performed confirms the initial statement, i.e. a $4 \\times 4$ general state is identified by a number of independent components given by $4^2-1=15$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reduction to 15 components\n",
    "\n",
    "pca_15 = PCA(n_components = 15)\n",
    "train_transf = pca_15.fit_transform(training_dt)\n",
    "test_transf = pca_15.fit_transform(test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s 16us/step\n",
      "5000/5000 [==============================] - 0s 15us/step\n",
      "Time to run the script: 350.84 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and test the previous perceptron on the reduced feature space\n",
    "\n",
    "start_time = time.time()\n",
    "pca_output = ff_net(train_transf, training_targets, test_transf, test_targets)\n",
    "print(\"Time to run the script: {} seconds\".format(round(time.time()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the PCA-transformed training set and on the PCA-transformed test set is 97.0 %, 97.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the PCA-transformed training set and on the PCA-transformed test set is {} %, {} %\".format(round(net_output[0],2)*100, round(net_output[1],2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA-transformed components are associated with the same classification accuracy as the non-transformed components. This result has been obtained both for Werner states and for general states and is due to the fact that the dimensional reduction is carried out with no overall variance loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Werner states can be learned with a perceptron at highest accuracy. This classification problem is linear, therefore it can be solved with a perceptron. \n",
    "\n",
    "When it comes to general quantum states, while our model may be still improved, on the other hand this might come with a higher computational cost due to a much larger network architecture, for example. Another difficulty of general quantum states is due to high dimensionality of the problem. Indeed, their entangled/separable boundary is 15-dimensional and highly non-linear. In spite of this, a structurally simple network as ours, composed of 2 hidden layers and 15 neurons/layer, has given a classification accuracy equal to 97\\% on the test data, within an execution time of about 5-6 minutes. Therefore, we should expect that an improvement of such model may be possible, increasing the number of neurons or choosing a deeper neural network. This solution should be applied taking into account some possible issues like overfitting, which may be prevented adding dropout layers for instance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
